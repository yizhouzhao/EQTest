{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MotionGPT import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = FBXDataLoader(\"FBXData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FBXDataLoader loading data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acae313d94a43b49d6bae6fee035258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=232.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loader.LoadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader.raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader.raw_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870f88e4d0074137a5331164bdfde748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=232.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loader.PrepareTrainingData(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2052"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader.train_data[180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 35, 66]) torch.Size([16, 35])\n",
      "torch.Size([16, 22, 66]) torch.Size([16, 22])\n",
      "torch.Size([16, 29, 66]) torch.Size([16, 29])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 27, 66]) torch.Size([16, 27])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 21, 66]) torch.Size([16, 21])\n",
      "torch.Size([16, 24, 66]) torch.Size([16, 24])\n",
      "torch.Size([16, 35, 66]) torch.Size([16, 35])\n",
      "torch.Size([16, 49, 66]) torch.Size([16, 49])\n",
      "torch.Size([16, 27, 66]) torch.Size([16, 27])\n",
      "torch.Size([16, 23, 66]) torch.Size([16, 23])\n",
      "torch.Size([16, 31, 66]) torch.Size([16, 31])\n",
      "torch.Size([16, 40, 66]) torch.Size([16, 40])\n",
      "torch.Size([16, 32, 66]) torch.Size([16, 32])\n",
      "torch.Size([16, 25, 66]) torch.Size([16, 25])\n",
      "torch.Size([16, 34, 66]) torch.Size([16, 34])\n",
      "torch.Size([16, 22, 66]) torch.Size([16, 22])\n",
      "torch.Size([16, 51, 66]) torch.Size([16, 51])\n",
      "torch.Size([16, 14, 66]) torch.Size([16, 14])\n",
      "torch.Size([16, 50, 66]) torch.Size([16, 50])\n",
      "torch.Size([16, 30, 66]) torch.Size([16, 30])\n",
      "torch.Size([16, 35, 66]) torch.Size([16, 35])\n",
      "torch.Size([16, 77, 66]) torch.Size([16, 77])\n",
      "torch.Size([16, 52, 66]) torch.Size([16, 52])\n",
      "torch.Size([16, 31, 66]) torch.Size([16, 31])\n",
      "torch.Size([16, 39, 66]) torch.Size([16, 39])\n",
      "torch.Size([16, 43, 66]) torch.Size([16, 43])\n",
      "torch.Size([16, 47, 66]) torch.Size([16, 47])\n",
      "torch.Size([16, 25, 66]) torch.Size([16, 25])\n",
      "torch.Size([16, 39, 66]) torch.Size([16, 39])\n",
      "torch.Size([16, 43, 66]) torch.Size([16, 43])\n",
      "torch.Size([16, 21, 66]) torch.Size([16, 21])\n",
      "torch.Size([16, 34, 66]) torch.Size([16, 34])\n",
      "torch.Size([16, 38, 66]) torch.Size([16, 38])\n",
      "torch.Size([16, 77, 66]) torch.Size([16, 77])\n",
      "torch.Size([16, 51, 66]) torch.Size([16, 51])\n",
      "torch.Size([16, 10, 66]) torch.Size([16, 10])\n",
      "torch.Size([16, 50, 66]) torch.Size([16, 50])\n",
      "torch.Size([16, 25, 66]) torch.Size([16, 25])\n",
      "torch.Size([16, 14, 66]) torch.Size([16, 14])\n",
      "torch.Size([16, 51, 66]) torch.Size([16, 51])\n",
      "torch.Size([16, 78, 66]) torch.Size([16, 78])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 22, 66]) torch.Size([16, 22])\n",
      "torch.Size([16, 38, 66]) torch.Size([16, 38])\n",
      "torch.Size([16, 22, 66]) torch.Size([16, 22])\n",
      "torch.Size([16, 51, 66]) torch.Size([16, 51])\n",
      "torch.Size([16, 51, 66]) torch.Size([16, 51])\n",
      "torch.Size([16, 22, 66]) torch.Size([16, 22])\n",
      "torch.Size([16, 47, 66]) torch.Size([16, 47])\n",
      "torch.Size([16, 39, 66]) torch.Size([16, 39])\n",
      "torch.Size([16, 43, 66]) torch.Size([16, 43])\n",
      "torch.Size([16, 27, 66]) torch.Size([16, 27])\n",
      "torch.Size([16, 49, 66]) torch.Size([16, 49])\n",
      "torch.Size([16, 47, 66]) torch.Size([16, 47])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 23, 66]) torch.Size([16, 23])\n",
      "torch.Size([16, 12, 66]) torch.Size([16, 12])\n",
      "torch.Size([16, 22, 66]) torch.Size([16, 22])\n",
      "torch.Size([16, 18, 66]) torch.Size([16, 18])\n",
      "torch.Size([16, 42, 66]) torch.Size([16, 42])\n",
      "torch.Size([16, 49, 66]) torch.Size([16, 49])\n",
      "torch.Size([16, 49, 66]) torch.Size([16, 49])\n",
      "torch.Size([16, 27, 66]) torch.Size([16, 27])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 47, 66]) torch.Size([16, 47])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 29, 66]) torch.Size([16, 29])\n",
      "torch.Size([16, 27, 66]) torch.Size([16, 27])\n",
      "torch.Size([16, 47, 66]) torch.Size([16, 47])\n",
      "torch.Size([16, 78, 66]) torch.Size([16, 78])\n",
      "torch.Size([16, 27, 66]) torch.Size([16, 27])\n",
      "torch.Size([16, 8, 66]) torch.Size([16, 8])\n",
      "torch.Size([16, 43, 66]) torch.Size([16, 43])\n",
      "torch.Size([16, 49, 66]) torch.Size([16, 49])\n",
      "torch.Size([16, 34, 66]) torch.Size([16, 34])\n",
      "torch.Size([16, 12, 66]) torch.Size([16, 12])\n",
      "torch.Size([16, 26, 66]) torch.Size([16, 26])\n",
      "torch.Size([16, 38, 66]) torch.Size([16, 38])\n",
      "torch.Size([16, 50, 66]) torch.Size([16, 50])\n",
      "torch.Size([16, 32, 66]) torch.Size([16, 32])\n",
      "torch.Size([16, 40, 66]) torch.Size([16, 40])\n",
      "torch.Size([16, 51, 66]) torch.Size([16, 51])\n",
      "torch.Size([16, 40, 66]) torch.Size([16, 40])\n",
      "torch.Size([16, 47, 66]) torch.Size([16, 47])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 49, 66]) torch.Size([16, 49])\n",
      "torch.Size([16, 40, 66]) torch.Size([16, 40])\n",
      "torch.Size([16, 78, 66]) torch.Size([16, 78])\n",
      "torch.Size([16, 39, 66]) torch.Size([16, 39])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 32, 66]) torch.Size([16, 32])\n",
      "torch.Size([16, 51, 66]) torch.Size([16, 51])\n",
      "torch.Size([16, 39, 66]) torch.Size([16, 39])\n",
      "torch.Size([16, 23, 66]) torch.Size([16, 23])\n",
      "torch.Size([16, 27, 66]) torch.Size([16, 27])\n",
      "torch.Size([16, 25, 66]) torch.Size([16, 25])\n",
      "torch.Size([16, 22, 66]) torch.Size([16, 22])\n",
      "torch.Size([16, 34, 66]) torch.Size([16, 34])\n",
      "torch.Size([16, 39, 66]) torch.Size([16, 39])\n",
      "torch.Size([16, 21, 66]) torch.Size([16, 21])\n",
      "torch.Size([16, 77, 66]) torch.Size([16, 77])\n",
      "torch.Size([16, 26, 66]) torch.Size([16, 26])\n",
      "torch.Size([16, 12, 66]) torch.Size([16, 12])\n",
      "torch.Size([16, 26, 66]) torch.Size([16, 26])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 33, 66]) torch.Size([16, 33])\n",
      "torch.Size([16, 21, 66]) torch.Size([16, 21])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 51, 66]) torch.Size([16, 51])\n",
      "torch.Size([16, 29, 66]) torch.Size([16, 29])\n",
      "torch.Size([16, 50, 66]) torch.Size([16, 50])\n",
      "torch.Size([16, 33, 66]) torch.Size([16, 33])\n",
      "torch.Size([16, 26, 66]) torch.Size([16, 26])\n",
      "torch.Size([16, 51, 66]) torch.Size([16, 51])\n",
      "torch.Size([16, 38, 66]) torch.Size([16, 38])\n",
      "torch.Size([16, 78, 66]) torch.Size([16, 78])\n",
      "torch.Size([16, 14, 66]) torch.Size([16, 14])\n",
      "torch.Size([16, 47, 66]) torch.Size([16, 47])\n",
      "torch.Size([16, 13, 66]) torch.Size([16, 13])\n",
      "torch.Size([16, 78, 66]) torch.Size([16, 78])\n",
      "torch.Size([16, 51, 66]) torch.Size([16, 51])\n",
      "torch.Size([16, 27, 66]) torch.Size([16, 27])\n",
      "torch.Size([16, 28, 66]) torch.Size([16, 28])\n",
      "torch.Size([16, 13, 66]) torch.Size([16, 13])\n",
      "torch.Size([16, 27, 66]) torch.Size([16, 27])\n",
      "torch.Size([4, 43, 66]) torch.Size([4, 43])\n"
     ]
    }
   ],
   "source": [
    "for bd,pd in loader.next_batch():\n",
    "    print(bd.shape, pd.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_data, pad_data = next(loader.next_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 35, 66])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 35])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VRNN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(loader.train_data[0][1])\n",
    "h_dim = 64\n",
    "z_dim = 10\n",
    "n_layers = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  #device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VRNN(\n",
       "  (phi_x): Sequential(\n",
       "    (0): Linear(in_features=66, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (phi_z): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (enc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (enc_mean): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (enc_std): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (prior): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (prior_mean): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (prior_std): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (dec): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (dec_std): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=66, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (dec_mean): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=66, bias=True)\n",
       "  )\n",
       "  (rnn): GRU(128, 64, num_layers=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VRNN(input_dim, h_dim, z_dim, n_layers, device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_epochs = 100\n",
    "clip = 10\n",
    "learning_rate = 1e-3\n",
    "print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\Transformer\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [160/2052 (0%)]\t KLD Loss: 0.775364 \t NLL Loss: 11284.482422\n",
      "Train Epoch: 0 [320/2052 (1%)]\t KLD Loss: 61.307632 \t NLL Loss: 46953.015625\n",
      "Train Epoch: 0 [480/2052 (1%)]\t KLD Loss: 640.820374 \t NLL Loss: 24631.425781\n",
      "Train Epoch: 0 [640/2052 (2%)]\t KLD Loss: 846.595398 \t NLL Loss: 4321.457520\n",
      "Train Epoch: 0 [800/2052 (2%)]\t KLD Loss: 544.751892 \t NLL Loss: 6188.418945\n",
      "Train Epoch: 0 [960/2052 (3%)]\t KLD Loss: 286.035034 \t NLL Loss: 7950.365723\n",
      "Train Epoch: 0 [1120/2052 (3%)]\t KLD Loss: 909.909241 \t NLL Loss: 71602.023438\n",
      "Train Epoch: 0 [1280/2052 (4%)]\t KLD Loss: 754.500366 \t NLL Loss: 6348.001953\n",
      "Train Epoch: 0 [1440/2052 (4%)]\t KLD Loss: 254.184784 \t NLL Loss: 22723.671875\n",
      "Train Epoch: 0 [1600/2052 (5%)]\t KLD Loss: 596.421448 \t NLL Loss: 394089.812500\n",
      "Train Epoch: 0 [1760/2052 (5%)]\t KLD Loss: 33.061310 \t NLL Loss: 1024.713379\n",
      "Train Epoch: 0 [1920/2052 (6%)]\t KLD Loss: 315.718536 \t NLL Loss: 21988.535156\n",
      "Train Epoch: 1 [160/2052 (0%)]\t KLD Loss: 25.335154 \t NLL Loss: 9682.433594\n",
      "Train Epoch: 1 [320/2052 (1%)]\t KLD Loss: 394.587769 \t NLL Loss: 17280.960938\n",
      "Train Epoch: 1 [480/2052 (1%)]\t KLD Loss: 151.088150 \t NLL Loss: 10262.988281\n",
      "Train Epoch: 1 [640/2052 (2%)]\t KLD Loss: 41.265659 \t NLL Loss: 2855.552246\n",
      "Train Epoch: 1 [800/2052 (2%)]\t KLD Loss: 76.071426 \t NLL Loss: 4386.235352\n",
      "Train Epoch: 1 [960/2052 (3%)]\t KLD Loss: 42.514633 \t NLL Loss: 7077.344727\n",
      "Train Epoch: 1 [1120/2052 (3%)]\t KLD Loss: 75.608284 \t NLL Loss: 70799.820312\n",
      "Train Epoch: 1 [1280/2052 (4%)]\t KLD Loss: 195.006866 \t NLL Loss: 4747.186035\n",
      "Train Epoch: 1 [1440/2052 (4%)]\t KLD Loss: 32.387436 \t NLL Loss: 22815.093750\n",
      "Train Epoch: 1 [1600/2052 (5%)]\t KLD Loss: 124.068947 \t NLL Loss: 392258.000000\n",
      "Train Epoch: 1 [1760/2052 (5%)]\t KLD Loss: 7.104585 \t NLL Loss: 1014.017822\n",
      "Train Epoch: 1 [1920/2052 (6%)]\t KLD Loss: 78.344635 \t NLL Loss: 21749.447266\n",
      "Train Epoch: 2 [160/2052 (0%)]\t KLD Loss: 7.415763 \t NLL Loss: 9320.561523\n",
      "Train Epoch: 2 [320/2052 (1%)]\t KLD Loss: 124.973358 \t NLL Loss: 17412.304688\n",
      "Train Epoch: 2 [480/2052 (1%)]\t KLD Loss: 59.284302 \t NLL Loss: 10032.558594\n",
      "Train Epoch: 2 [640/2052 (2%)]\t KLD Loss: 14.919380 \t NLL Loss: 2792.809814\n",
      "Train Epoch: 2 [800/2052 (2%)]\t KLD Loss: 24.416937 \t NLL Loss: 4237.203125\n",
      "Train Epoch: 2 [960/2052 (3%)]\t KLD Loss: 15.814939 \t NLL Loss: 7135.744629\n",
      "Train Epoch: 2 [1120/2052 (3%)]\t KLD Loss: 25.216837 \t NLL Loss: 70595.500000\n",
      "Train Epoch: 2 [1280/2052 (4%)]\t KLD Loss: 96.859589 \t NLL Loss: 4709.700195\n",
      "Train Epoch: 2 [1440/2052 (4%)]\t KLD Loss: 16.706015 \t NLL Loss: 22794.316406\n",
      "Train Epoch: 2 [1600/2052 (5%)]\t KLD Loss: 41.368706 \t NLL Loss: 391106.312500\n",
      "Train Epoch: 2 [1760/2052 (5%)]\t KLD Loss: 2.418443 \t NLL Loss: 1041.554199\n",
      "Train Epoch: 2 [1920/2052 (6%)]\t KLD Loss: 33.939461 \t NLL Loss: 21866.253906\n",
      "Train Epoch: 3 [160/2052 (0%)]\t KLD Loss: 4.814409 \t NLL Loss: 9341.549805\n",
      "Train Epoch: 3 [320/2052 (1%)]\t KLD Loss: 55.243916 \t NLL Loss: 17457.287109\n",
      "Train Epoch: 3 [480/2052 (1%)]\t KLD Loss: 29.273214 \t NLL Loss: 10030.780273\n",
      "Train Epoch: 3 [640/2052 (2%)]\t KLD Loss: 8.847218 \t NLL Loss: 2793.798828\n",
      "Train Epoch: 3 [800/2052 (2%)]\t KLD Loss: 15.553541 \t NLL Loss: 4344.996094\n",
      "Train Epoch: 3 [960/2052 (3%)]\t KLD Loss: 10.507248 \t NLL Loss: 7143.562012\n",
      "Train Epoch: 3 [1120/2052 (3%)]\t KLD Loss: 18.042622 \t NLL Loss: 70531.351562\n",
      "Train Epoch: 3 [1280/2052 (4%)]\t KLD Loss: 57.908825 \t NLL Loss: 4757.474121\n",
      "Train Epoch: 3 [1440/2052 (4%)]\t KLD Loss: 9.640495 \t NLL Loss: 22756.693359\n",
      "Train Epoch: 3 [1600/2052 (5%)]\t KLD Loss: 26.408291 \t NLL Loss: 390561.687500\n",
      "Train Epoch: 3 [1760/2052 (5%)]\t KLD Loss: 2.350502 \t NLL Loss: 1024.427734\n",
      "Train Epoch: 3 [1920/2052 (6%)]\t KLD Loss: 29.444756 \t NLL Loss: 21790.851562\n",
      "Train Epoch: 4 [160/2052 (0%)]\t KLD Loss: 2.951526 \t NLL Loss: 9310.118164\n",
      "Train Epoch: 4 [320/2052 (1%)]\t KLD Loss: 47.364304 \t NLL Loss: 17426.972656\n",
      "Train Epoch: 4 [480/2052 (1%)]\t KLD Loss: 23.892937 \t NLL Loss: 10063.427734\n",
      "Train Epoch: 4 [640/2052 (2%)]\t KLD Loss: 6.221008 \t NLL Loss: 2769.972656\n",
      "Train Epoch: 4 [800/2052 (2%)]\t KLD Loss: 9.943984 \t NLL Loss: 4207.450195\n",
      "Train Epoch: 4 [960/2052 (3%)]\t KLD Loss: 6.359190 \t NLL Loss: 7222.764160\n",
      "Train Epoch: 4 [1120/2052 (3%)]\t KLD Loss: 12.445054 \t NLL Loss: 70474.445312\n",
      "Train Epoch: 4 [1280/2052 (4%)]\t KLD Loss: 38.729477 \t NLL Loss: 4840.326172\n",
      "Train Epoch: 4 [1440/2052 (4%)]\t KLD Loss: 6.863828 \t NLL Loss: 22796.330078\n",
      "Train Epoch: 4 [1600/2052 (5%)]\t KLD Loss: 25.247417 \t NLL Loss: 391047.750000\n",
      "Train Epoch: 4 [1760/2052 (5%)]\t KLD Loss: 1.979308 \t NLL Loss: 1028.905151\n",
      "Train Epoch: 4 [1920/2052 (6%)]\t KLD Loss: 23.383976 \t NLL Loss: 21792.625000\n",
      "Train Epoch: 5 [160/2052 (0%)]\t KLD Loss: 2.546587 \t NLL Loss: 9307.353516\n",
      "Train Epoch: 5 [320/2052 (1%)]\t KLD Loss: 39.242523 \t NLL Loss: 17451.109375\n",
      "Train Epoch: 5 [480/2052 (1%)]\t KLD Loss: 20.524914 \t NLL Loss: 10035.052734\n",
      "Train Epoch: 5 [640/2052 (2%)]\t KLD Loss: 4.574717 \t NLL Loss: 2784.019531\n",
      "Train Epoch: 5 [800/2052 (2%)]\t KLD Loss: 7.634155 \t NLL Loss: 4253.994629\n",
      "Train Epoch: 5 [960/2052 (3%)]\t KLD Loss: 5.386105 \t NLL Loss: 7200.103516\n",
      "Train Epoch: 5 [1120/2052 (3%)]\t KLD Loss: 11.583172 \t NLL Loss: 70412.789062\n",
      "Train Epoch: 5 [1280/2052 (4%)]\t KLD Loss: 34.155834 \t NLL Loss: 4776.208496\n",
      "Train Epoch: 5 [1440/2052 (4%)]\t KLD Loss: 5.991461 \t NLL Loss: 22714.855469\n",
      "Train Epoch: 5 [1600/2052 (5%)]\t KLD Loss: 19.935020 \t NLL Loss: 390474.187500\n",
      "Train Epoch: 5 [1760/2052 (5%)]\t KLD Loss: 1.619584 \t NLL Loss: 1025.084717\n",
      "Train Epoch: 5 [1920/2052 (6%)]\t KLD Loss: 17.025730 \t NLL Loss: 21946.640625\n",
      "Train Epoch: 6 [160/2052 (0%)]\t KLD Loss: 2.427083 \t NLL Loss: 9303.066406\n",
      "Train Epoch: 6 [320/2052 (1%)]\t KLD Loss: 29.676569 \t NLL Loss: 17323.097656\n",
      "Train Epoch: 6 [480/2052 (1%)]\t KLD Loss: 15.985741 \t NLL Loss: 10003.430664\n",
      "Train Epoch: 6 [640/2052 (2%)]\t KLD Loss: 4.163159 \t NLL Loss: 2769.036133\n",
      "Train Epoch: 6 [800/2052 (2%)]\t KLD Loss: 6.588433 \t NLL Loss: 4254.423340\n",
      "Train Epoch: 6 [960/2052 (3%)]\t KLD Loss: 4.231192 \t NLL Loss: 7220.454102\n",
      "Train Epoch: 6 [1120/2052 (3%)]\t KLD Loss: 10.309528 \t NLL Loss: 70385.312500\n",
      "Train Epoch: 6 [1280/2052 (4%)]\t KLD Loss: 28.604563 \t NLL Loss: 4775.044922\n",
      "Train Epoch: 6 [1440/2052 (4%)]\t KLD Loss: 6.016032 \t NLL Loss: 22739.289062\n",
      "Train Epoch: 6 [1600/2052 (5%)]\t KLD Loss: 19.713955 \t NLL Loss: 390059.375000\n",
      "Train Epoch: 6 [1760/2052 (5%)]\t KLD Loss: 1.660617 \t NLL Loss: 1029.463989\n",
      "Train Epoch: 6 [1920/2052 (6%)]\t KLD Loss: 19.323593 \t NLL Loss: 21804.445312\n",
      "Train Epoch: 7 [160/2052 (0%)]\t KLD Loss: 4.034650 \t NLL Loss: 9220.746094\n",
      "Train Epoch: 7 [320/2052 (1%)]\t KLD Loss: 28.552925 \t NLL Loss: 16892.482422\n",
      "Train Epoch: 7 [480/2052 (1%)]\t KLD Loss: 25.248636 \t NLL Loss: 9874.196289\n",
      "Train Epoch: 7 [640/2052 (2%)]\t KLD Loss: 4.767193 \t NLL Loss: 2795.743896\n",
      "Train Epoch: 7 [800/2052 (2%)]\t KLD Loss: 7.803004 \t NLL Loss: 4227.038086\n",
      "Train Epoch: 7 [960/2052 (3%)]\t KLD Loss: 4.880364 \t NLL Loss: 7229.695801\n",
      "Train Epoch: 7 [1120/2052 (3%)]\t KLD Loss: 14.463381 \t NLL Loss: 70225.960938\n",
      "Train Epoch: 7 [1280/2052 (4%)]\t KLD Loss: 33.029549 \t NLL Loss: 5058.218750\n",
      "Train Epoch: 7 [1440/2052 (4%)]\t KLD Loss: 8.341125 \t NLL Loss: 22695.101562\n",
      "Train Epoch: 7 [1600/2052 (5%)]\t KLD Loss: 36.490452 \t NLL Loss: 390072.875000\n",
      "Train Epoch: 7 [1760/2052 (5%)]\t KLD Loss: 2.592297 \t NLL Loss: 992.640991\n",
      "Train Epoch: 7 [1920/2052 (6%)]\t KLD Loss: 35.658611 \t NLL Loss: 20983.798828\n",
      "Train Epoch: 8 [160/2052 (0%)]\t KLD Loss: 9.187403 \t NLL Loss: 8874.429688\n",
      "Train Epoch: 8 [320/2052 (1%)]\t KLD Loss: 71.496109 \t NLL Loss: 14095.967773\n",
      "Train Epoch: 8 [480/2052 (1%)]\t KLD Loss: 34.001259 \t NLL Loss: 9728.046875\n",
      "Train Epoch: 8 [640/2052 (2%)]\t KLD Loss: 5.486631 \t NLL Loss: 2752.640869\n",
      "Train Epoch: 8 [800/2052 (2%)]\t KLD Loss: 6.839097 \t NLL Loss: 4603.101074\n",
      "Train Epoch: 8 [960/2052 (3%)]\t KLD Loss: 7.692794 \t NLL Loss: 6236.168945\n",
      "Train Epoch: 8 [1120/2052 (3%)]\t KLD Loss: 16.678049 \t NLL Loss: 70477.546875\n",
      "Train Epoch: 8 [1280/2052 (4%)]\t KLD Loss: 51.805035 \t NLL Loss: 5738.253906\n",
      "Train Epoch: 8 [1440/2052 (4%)]\t KLD Loss: 6.266551 \t NLL Loss: 22703.656250\n",
      "Train Epoch: 8 [1600/2052 (5%)]\t KLD Loss: 45.525352 \t NLL Loss: 386504.062500\n",
      "Train Epoch: 8 [1760/2052 (5%)]\t KLD Loss: 3.010660 \t NLL Loss: 930.436340\n",
      "Train Epoch: 8 [1920/2052 (6%)]\t KLD Loss: 32.889618 \t NLL Loss: 20130.792969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [160/2052 (0%)]\t KLD Loss: 6.784226 \t NLL Loss: 8458.011719\n",
      "Train Epoch: 9 [320/2052 (1%)]\t KLD Loss: 48.419247 \t NLL Loss: 13470.113281\n",
      "Train Epoch: 9 [480/2052 (1%)]\t KLD Loss: 29.720669 \t NLL Loss: 9024.613281\n",
      "Train Epoch: 9 [640/2052 (2%)]\t KLD Loss: 5.617400 \t NLL Loss: 2699.017822\n",
      "Train Epoch: 9 [800/2052 (2%)]\t KLD Loss: 7.611531 \t NLL Loss: 4131.024902\n",
      "Train Epoch: 9 [960/2052 (3%)]\t KLD Loss: 16.264359 \t NLL Loss: 5613.184570\n",
      "Train Epoch: 9 [1120/2052 (3%)]\t KLD Loss: 16.467695 \t NLL Loss: 69748.625000\n",
      "Train Epoch: 9 [1280/2052 (4%)]\t KLD Loss: 28.231785 \t NLL Loss: 4300.016113\n",
      "Train Epoch: 9 [1440/2052 (4%)]\t KLD Loss: 8.500415 \t NLL Loss: 22605.949219\n",
      "Train Epoch: 9 [1600/2052 (5%)]\t KLD Loss: 78.848373 \t NLL Loss: 385466.687500\n",
      "Train Epoch: 9 [1760/2052 (5%)]\t KLD Loss: 2.881235 \t NLL Loss: 892.759766\n",
      "Train Epoch: 9 [1920/2052 (6%)]\t KLD Loss: 40.571602 \t NLL Loss: 18798.976562\n",
      "Train Epoch: 10 [160/2052 (0%)]\t KLD Loss: 14.376349 \t NLL Loss: 6894.723145\n",
      "Train Epoch: 10 [320/2052 (1%)]\t KLD Loss: 42.179893 \t NLL Loss: 13312.822266\n",
      "Train Epoch: 10 [480/2052 (1%)]\t KLD Loss: 27.418175 \t NLL Loss: 8611.145508\n",
      "Train Epoch: 10 [640/2052 (2%)]\t KLD Loss: 5.937813 \t NLL Loss: 2692.702393\n",
      "Train Epoch: 10 [800/2052 (2%)]\t KLD Loss: 8.142838 \t NLL Loss: 4063.641113\n",
      "Train Epoch: 10 [960/2052 (3%)]\t KLD Loss: 18.580482 \t NLL Loss: 5254.976562\n",
      "Train Epoch: 10 [1120/2052 (3%)]\t KLD Loss: 21.759661 \t NLL Loss: 69161.140625\n",
      "Train Epoch: 10 [1280/2052 (4%)]\t KLD Loss: 29.505640 \t NLL Loss: 4777.266602\n",
      "Train Epoch: 10 [1440/2052 (4%)]\t KLD Loss: 8.521197 \t NLL Loss: 22416.089844\n",
      "Train Epoch: 10 [1600/2052 (5%)]\t KLD Loss: 79.506569 \t NLL Loss: 385030.156250\n",
      "Train Epoch: 10 [1760/2052 (5%)]\t KLD Loss: 2.647501 \t NLL Loss: 880.534546\n",
      "Train Epoch: 10 [1920/2052 (6%)]\t KLD Loss: 41.255970 \t NLL Loss: 16813.933594\n",
      "Train Epoch: 11 [160/2052 (0%)]\t KLD Loss: 14.596231 \t NLL Loss: 7003.625000\n",
      "Train Epoch: 11 [320/2052 (1%)]\t KLD Loss: 31.620296 \t NLL Loss: 13637.656250\n",
      "Train Epoch: 11 [480/2052 (1%)]\t KLD Loss: 27.924034 \t NLL Loss: 8603.524414\n",
      "Train Epoch: 11 [640/2052 (2%)]\t KLD Loss: 5.779818 \t NLL Loss: 2622.832031\n",
      "Train Epoch: 11 [800/2052 (2%)]\t KLD Loss: 6.448201 \t NLL Loss: 3915.437012\n",
      "Train Epoch: 11 [960/2052 (3%)]\t KLD Loss: 19.224054 \t NLL Loss: 4998.955566\n",
      "Train Epoch: 11 [1120/2052 (3%)]\t KLD Loss: 18.564682 \t NLL Loss: 69013.906250\n",
      "Train Epoch: 11 [1280/2052 (4%)]\t KLD Loss: 26.334183 \t NLL Loss: 5038.317383\n",
      "Train Epoch: 11 [1440/2052 (4%)]\t KLD Loss: 13.475614 \t NLL Loss: 22075.695312\n",
      "Train Epoch: 11 [1600/2052 (5%)]\t KLD Loss: 50.960831 \t NLL Loss: 383748.437500\n",
      "Train Epoch: 11 [1760/2052 (5%)]\t KLD Loss: 2.914702 \t NLL Loss: 897.932617\n",
      "Train Epoch: 11 [1920/2052 (6%)]\t KLD Loss: 42.860268 \t NLL Loss: 16277.011719\n",
      "Train Epoch: 12 [160/2052 (0%)]\t KLD Loss: 20.335470 \t NLL Loss: 7716.503906\n",
      "Train Epoch: 12 [320/2052 (1%)]\t KLD Loss: 29.731350 \t NLL Loss: 13658.857422\n",
      "Train Epoch: 12 [480/2052 (1%)]\t KLD Loss: 27.405888 \t NLL Loss: 8431.050781\n",
      "Train Epoch: 12 [640/2052 (2%)]\t KLD Loss: 5.735826 \t NLL Loss: 2599.270996\n",
      "Train Epoch: 12 [800/2052 (2%)]\t KLD Loss: 6.170834 \t NLL Loss: 3879.570068\n",
      "Train Epoch: 12 [960/2052 (3%)]\t KLD Loss: 19.921770 \t NLL Loss: 4786.593262\n",
      "Train Epoch: 12 [1120/2052 (3%)]\t KLD Loss: 18.565189 \t NLL Loss: 68596.312500\n",
      "Train Epoch: 12 [1280/2052 (4%)]\t KLD Loss: 27.536226 \t NLL Loss: 4803.459961\n",
      "Train Epoch: 12 [1440/2052 (4%)]\t KLD Loss: 14.760370 \t NLL Loss: 21324.359375\n",
      "Train Epoch: 12 [1600/2052 (5%)]\t KLD Loss: 120.486572 \t NLL Loss: 382188.312500\n",
      "Train Epoch: 12 [1760/2052 (5%)]\t KLD Loss: 3.066710 \t NLL Loss: 920.878906\n",
      "Train Epoch: 12 [1920/2052 (6%)]\t KLD Loss: 45.400200 \t NLL Loss: 16049.626953\n",
      "Train Epoch: 13 [160/2052 (0%)]\t KLD Loss: 9.005188 \t NLL Loss: 7131.687500\n",
      "Train Epoch: 13 [320/2052 (1%)]\t KLD Loss: 38.914764 \t NLL Loss: 13925.146484\n",
      "Train Epoch: 13 [480/2052 (1%)]\t KLD Loss: 30.855862 \t NLL Loss: 8099.886719\n",
      "Train Epoch: 13 [640/2052 (2%)]\t KLD Loss: 6.532613 \t NLL Loss: 2541.309570\n",
      "Train Epoch: 13 [800/2052 (2%)]\t KLD Loss: 7.584044 \t NLL Loss: 3871.887451\n",
      "Train Epoch: 13 [960/2052 (3%)]\t KLD Loss: 29.025764 \t NLL Loss: 6068.093750\n",
      "Train Epoch: 13 [1120/2052 (3%)]\t KLD Loss: 20.153448 \t NLL Loss: 68238.617188\n",
      "Train Epoch: 13 [1280/2052 (4%)]\t KLD Loss: 27.158398 \t NLL Loss: 4671.849121\n",
      "Train Epoch: 13 [1440/2052 (4%)]\t KLD Loss: 15.088961 \t NLL Loss: 20706.451172\n",
      "Train Epoch: 13 [1600/2052 (5%)]\t KLD Loss: 68.458862 \t NLL Loss: 378208.500000\n",
      "Train Epoch: 13 [1760/2052 (5%)]\t KLD Loss: 3.160901 \t NLL Loss: 987.316528\n",
      "Train Epoch: 13 [1920/2052 (6%)]\t KLD Loss: 48.118656 \t NLL Loss: 16161.837891\n",
      "Train Epoch: 14 [160/2052 (0%)]\t KLD Loss: 12.247601 \t NLL Loss: 7132.702148\n",
      "Train Epoch: 14 [320/2052 (1%)]\t KLD Loss: 31.763084 \t NLL Loss: 13749.359375\n",
      "Train Epoch: 14 [480/2052 (1%)]\t KLD Loss: 28.451698 \t NLL Loss: 7822.119141\n",
      "Train Epoch: 14 [640/2052 (2%)]\t KLD Loss: 6.448884 \t NLL Loss: 2543.258789\n",
      "Train Epoch: 14 [800/2052 (2%)]\t KLD Loss: 6.935187 \t NLL Loss: 3901.928711\n",
      "Train Epoch: 14 [960/2052 (3%)]\t KLD Loss: 24.758881 \t NLL Loss: 4558.157715\n",
      "Train Epoch: 14 [1120/2052 (3%)]\t KLD Loss: 20.207207 \t NLL Loss: 68157.234375\n",
      "Train Epoch: 14 [1280/2052 (4%)]\t KLD Loss: 29.232843 \t NLL Loss: 4338.735840\n",
      "Train Epoch: 14 [1440/2052 (4%)]\t KLD Loss: 15.042940 \t NLL Loss: 20390.150391\n",
      "Train Epoch: 14 [1600/2052 (5%)]\t KLD Loss: 87.670738 \t NLL Loss: 374597.125000\n",
      "Train Epoch: 14 [1760/2052 (5%)]\t KLD Loss: 3.235164 \t NLL Loss: 979.782043\n",
      "Train Epoch: 14 [1920/2052 (6%)]\t KLD Loss: 40.689117 \t NLL Loss: 15402.583984\n",
      "Train Epoch: 15 [160/2052 (0%)]\t KLD Loss: 10.020693 \t NLL Loss: 7137.428223\n",
      "Train Epoch: 15 [320/2052 (1%)]\t KLD Loss: 33.617584 \t NLL Loss: 13727.128906\n",
      "Train Epoch: 15 [480/2052 (1%)]\t KLD Loss: 26.409893 \t NLL Loss: 7595.942383\n",
      "Train Epoch: 15 [640/2052 (2%)]\t KLD Loss: 5.489381 \t NLL Loss: 2526.077148\n",
      "Train Epoch: 15 [800/2052 (2%)]\t KLD Loss: 5.920494 \t NLL Loss: 3934.355469\n",
      "Train Epoch: 15 [960/2052 (3%)]\t KLD Loss: 19.036915 \t NLL Loss: 3950.854980\n",
      "Train Epoch: 15 [1120/2052 (3%)]\t KLD Loss: 21.188198 \t NLL Loss: 67868.796875\n",
      "Train Epoch: 15 [1280/2052 (4%)]\t KLD Loss: 30.719105 \t NLL Loss: 4261.981445\n",
      "Train Epoch: 15 [1440/2052 (4%)]\t KLD Loss: 12.124434 \t NLL Loss: 19895.828125\n",
      "Train Epoch: 15 [1600/2052 (5%)]\t KLD Loss: 79.055779 \t NLL Loss: 372795.187500\n",
      "Train Epoch: 15 [1760/2052 (5%)]\t KLD Loss: 2.749132 \t NLL Loss: 935.911072\n",
      "Train Epoch: 15 [1920/2052 (6%)]\t KLD Loss: 39.451096 \t NLL Loss: 14931.188477\n",
      "Train Epoch: 16 [160/2052 (0%)]\t KLD Loss: 7.736959 \t NLL Loss: 7296.083984\n",
      "Train Epoch: 16 [320/2052 (1%)]\t KLD Loss: 30.748810 \t NLL Loss: 13725.103516\n",
      "Train Epoch: 16 [480/2052 (1%)]\t KLD Loss: 26.834564 \t NLL Loss: 7300.642578\n",
      "Train Epoch: 16 [640/2052 (2%)]\t KLD Loss: 6.035574 \t NLL Loss: 2511.547363\n",
      "Train Epoch: 16 [800/2052 (2%)]\t KLD Loss: 7.329638 \t NLL Loss: 3903.394531\n",
      "Train Epoch: 16 [960/2052 (3%)]\t KLD Loss: 22.333157 \t NLL Loss: 3744.251221\n",
      "Train Epoch: 16 [1120/2052 (3%)]\t KLD Loss: 20.513821 \t NLL Loss: 67944.820312\n",
      "Train Epoch: 16 [1280/2052 (4%)]\t KLD Loss: 30.158205 \t NLL Loss: 4199.876465\n",
      "Train Epoch: 16 [1440/2052 (4%)]\t KLD Loss: 13.992542 \t NLL Loss: 19451.289062\n",
      "Train Epoch: 16 [1600/2052 (5%)]\t KLD Loss: 94.668030 \t NLL Loss: 373659.375000\n",
      "Train Epoch: 16 [1760/2052 (5%)]\t KLD Loss: 3.041573 \t NLL Loss: 894.585754\n",
      "Train Epoch: 16 [1920/2052 (6%)]\t KLD Loss: 39.552807 \t NLL Loss: 14284.410156\n",
      "Train Epoch: 17 [160/2052 (0%)]\t KLD Loss: 6.902359 \t NLL Loss: 7534.321777\n",
      "Train Epoch: 17 [320/2052 (1%)]\t KLD Loss: 37.002430 \t NLL Loss: 14415.931641\n",
      "Train Epoch: 17 [480/2052 (1%)]\t KLD Loss: 26.014561 \t NLL Loss: 7288.643555\n",
      "Train Epoch: 17 [640/2052 (2%)]\t KLD Loss: 5.464092 \t NLL Loss: 2558.268555\n",
      "Train Epoch: 17 [800/2052 (2%)]\t KLD Loss: 7.113504 \t NLL Loss: 3800.982910\n",
      "Train Epoch: 17 [960/2052 (3%)]\t KLD Loss: 28.749538 \t NLL Loss: 6119.202637\n",
      "Train Epoch: 17 [1120/2052 (3%)]\t KLD Loss: 22.081411 \t NLL Loss: 67939.171875\n",
      "Train Epoch: 17 [1280/2052 (4%)]\t KLD Loss: 28.938099 \t NLL Loss: 4123.607910\n",
      "Train Epoch: 17 [1440/2052 (4%)]\t KLD Loss: 13.488724 \t NLL Loss: 19005.625000\n",
      "Train Epoch: 17 [1600/2052 (5%)]\t KLD Loss: 65.839432 \t NLL Loss: 372215.437500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [1760/2052 (5%)]\t KLD Loss: 3.297612 \t NLL Loss: 937.329468\n",
      "Train Epoch: 17 [1920/2052 (6%)]\t KLD Loss: 39.766350 \t NLL Loss: 13618.395508\n",
      "Train Epoch: 18 [160/2052 (0%)]\t KLD Loss: 7.614186 \t NLL Loss: 7705.498535\n",
      "Train Epoch: 18 [320/2052 (1%)]\t KLD Loss: 32.444508 \t NLL Loss: 14614.883789\n",
      "Train Epoch: 18 [480/2052 (1%)]\t KLD Loss: 25.191420 \t NLL Loss: 7351.859375\n",
      "Train Epoch: 18 [640/2052 (2%)]\t KLD Loss: 5.668165 \t NLL Loss: 2528.207520\n",
      "Train Epoch: 18 [800/2052 (2%)]\t KLD Loss: 6.580755 \t NLL Loss: 3792.726074\n",
      "Train Epoch: 18 [960/2052 (3%)]\t KLD Loss: 25.991173 \t NLL Loss: 5087.053711\n",
      "Train Epoch: 18 [1120/2052 (3%)]\t KLD Loss: 18.937611 \t NLL Loss: 67813.671875\n",
      "Train Epoch: 18 [1280/2052 (4%)]\t KLD Loss: 25.289135 \t NLL Loss: 4065.602295\n",
      "Train Epoch: 18 [1440/2052 (4%)]\t KLD Loss: 12.512001 \t NLL Loss: 18677.523438\n",
      "Train Epoch: 18 [1600/2052 (5%)]\t KLD Loss: 83.939514 \t NLL Loss: 370006.968750\n",
      "Train Epoch: 18 [1760/2052 (5%)]\t KLD Loss: 3.138979 \t NLL Loss: 838.734253\n",
      "Train Epoch: 18 [1920/2052 (6%)]\t KLD Loss: 37.422150 \t NLL Loss: 13278.877930\n",
      "Train Epoch: 19 [160/2052 (0%)]\t KLD Loss: 6.205491 \t NLL Loss: 7657.490234\n",
      "Train Epoch: 19 [320/2052 (1%)]\t KLD Loss: 31.202879 \t NLL Loss: 14953.019531\n",
      "Train Epoch: 19 [480/2052 (1%)]\t KLD Loss: 26.920834 \t NLL Loss: 7385.920410\n",
      "Train Epoch: 19 [640/2052 (2%)]\t KLD Loss: 5.936848 \t NLL Loss: 2531.375977\n",
      "Train Epoch: 19 [800/2052 (2%)]\t KLD Loss: 6.538166 \t NLL Loss: 3759.268799\n",
      "Train Epoch: 19 [960/2052 (3%)]\t KLD Loss: 26.032375 \t NLL Loss: 4744.918945\n",
      "Train Epoch: 19 [1120/2052 (3%)]\t KLD Loss: 22.557886 \t NLL Loss: 67258.101562\n",
      "Train Epoch: 19 [1280/2052 (4%)]\t KLD Loss: 29.266916 \t NLL Loss: 4254.747559\n",
      "Train Epoch: 19 [1440/2052 (4%)]\t KLD Loss: 14.792784 \t NLL Loss: 18384.824219\n",
      "Train Epoch: 19 [1600/2052 (5%)]\t KLD Loss: 88.442871 \t NLL Loss: 371108.750000\n",
      "Train Epoch: 19 [1760/2052 (5%)]\t KLD Loss: 3.192253 \t NLL Loss: 818.349670\n",
      "Train Epoch: 19 [1920/2052 (6%)]\t KLD Loss: 37.873905 \t NLL Loss: 13429.509766\n",
      "Train Epoch: 20 [160/2052 (0%)]\t KLD Loss: 6.089881 \t NLL Loss: 7676.097168\n",
      "Train Epoch: 20 [320/2052 (1%)]\t KLD Loss: 32.190117 \t NLL Loss: 16527.574219\n",
      "Train Epoch: 20 [480/2052 (1%)]\t KLD Loss: 25.978966 \t NLL Loss: 7109.202148\n",
      "Train Epoch: 20 [640/2052 (2%)]\t KLD Loss: 6.235904 \t NLL Loss: 2491.185059\n",
      "Train Epoch: 20 [800/2052 (2%)]\t KLD Loss: 6.510663 \t NLL Loss: 3702.880371\n",
      "Train Epoch: 20 [960/2052 (3%)]\t KLD Loss: 15.631841 \t NLL Loss: 4134.080078\n",
      "Train Epoch: 20 [1120/2052 (3%)]\t KLD Loss: 20.978214 \t NLL Loss: 67182.195312\n",
      "Train Epoch: 20 [1280/2052 (4%)]\t KLD Loss: 25.863861 \t NLL Loss: 4232.623535\n",
      "Train Epoch: 20 [1440/2052 (4%)]\t KLD Loss: 13.134790 \t NLL Loss: 17618.068359\n",
      "Train Epoch: 20 [1600/2052 (5%)]\t KLD Loss: 90.970772 \t NLL Loss: 372739.156250\n",
      "Train Epoch: 20 [1760/2052 (5%)]\t KLD Loss: 2.808785 \t NLL Loss: 789.762451\n",
      "Train Epoch: 20 [1920/2052 (6%)]\t KLD Loss: 34.373283 \t NLL Loss: 13014.937500\n",
      "Train Epoch: 21 [160/2052 (0%)]\t KLD Loss: 5.695608 \t NLL Loss: 7902.708496\n",
      "Train Epoch: 21 [320/2052 (1%)]\t KLD Loss: 28.460777 \t NLL Loss: 15424.167969\n",
      "Train Epoch: 21 [480/2052 (1%)]\t KLD Loss: 27.711161 \t NLL Loss: 7258.300781\n",
      "Train Epoch: 21 [640/2052 (2%)]\t KLD Loss: 6.624701 \t NLL Loss: 2468.550781\n",
      "Train Epoch: 21 [800/2052 (2%)]\t KLD Loss: 6.988620 \t NLL Loss: 3614.543701\n",
      "Train Epoch: 21 [960/2052 (3%)]\t KLD Loss: 16.501974 \t NLL Loss: 3661.341797\n",
      "Train Epoch: 21 [1120/2052 (3%)]\t KLD Loss: 22.655613 \t NLL Loss: 66872.460938\n",
      "Train Epoch: 21 [1280/2052 (4%)]\t KLD Loss: 28.341917 \t NLL Loss: 4409.989258\n",
      "Train Epoch: 21 [1440/2052 (4%)]\t KLD Loss: 12.226831 \t NLL Loss: 17380.537109\n",
      "Train Epoch: 21 [1600/2052 (5%)]\t KLD Loss: 111.319984 \t NLL Loss: 372245.937500\n",
      "Train Epoch: 21 [1760/2052 (5%)]\t KLD Loss: 3.001095 \t NLL Loss: 724.682739\n",
      "Train Epoch: 21 [1920/2052 (6%)]\t KLD Loss: 34.737549 \t NLL Loss: 12761.185547\n",
      "Train Epoch: 22 [160/2052 (0%)]\t KLD Loss: 5.678087 \t NLL Loss: 7935.083008\n",
      "Train Epoch: 22 [320/2052 (1%)]\t KLD Loss: 28.303562 \t NLL Loss: 15396.269531\n",
      "Train Epoch: 22 [480/2052 (1%)]\t KLD Loss: 26.791939 \t NLL Loss: 7405.593262\n",
      "Train Epoch: 22 [640/2052 (2%)]\t KLD Loss: 6.761070 \t NLL Loss: 2407.534668\n",
      "Train Epoch: 22 [800/2052 (2%)]\t KLD Loss: 7.711721 \t NLL Loss: 3598.697021\n",
      "Train Epoch: 22 [960/2052 (3%)]\t KLD Loss: 24.475822 \t NLL Loss: 3642.441162\n",
      "Train Epoch: 22 [1120/2052 (3%)]\t KLD Loss: 26.910213 \t NLL Loss: 66514.828125\n",
      "Train Epoch: 22 [1280/2052 (4%)]\t KLD Loss: 25.265480 \t NLL Loss: 4245.276367\n",
      "Train Epoch: 22 [1440/2052 (4%)]\t KLD Loss: 14.626859 \t NLL Loss: 17048.330078\n",
      "Train Epoch: 22 [1600/2052 (5%)]\t KLD Loss: 70.280136 \t NLL Loss: 371197.781250\n",
      "Train Epoch: 22 [1760/2052 (5%)]\t KLD Loss: 3.387634 \t NLL Loss: 699.298889\n",
      "Train Epoch: 22 [1920/2052 (6%)]\t KLD Loss: 34.184967 \t NLL Loss: 13154.666016\n",
      "Train Epoch: 23 [160/2052 (0%)]\t KLD Loss: 5.214478 \t NLL Loss: 8039.534668\n",
      "Train Epoch: 23 [320/2052 (1%)]\t KLD Loss: 24.238712 \t NLL Loss: 15914.958008\n",
      "Train Epoch: 23 [480/2052 (1%)]\t KLD Loss: 25.199831 \t NLL Loss: 7370.780762\n",
      "Train Epoch: 23 [640/2052 (2%)]\t KLD Loss: 5.900514 \t NLL Loss: 2385.899902\n",
      "Train Epoch: 23 [800/2052 (2%)]\t KLD Loss: 6.257652 \t NLL Loss: 3503.651367\n",
      "Train Epoch: 23 [960/2052 (3%)]\t KLD Loss: 25.921305 \t NLL Loss: 4713.704590\n",
      "Train Epoch: 23 [1120/2052 (3%)]\t KLD Loss: 22.978109 \t NLL Loss: 66087.617188\n",
      "Train Epoch: 23 [1280/2052 (4%)]\t KLD Loss: 22.712666 \t NLL Loss: 4620.827148\n",
      "Train Epoch: 23 [1440/2052 (4%)]\t KLD Loss: 14.743523 \t NLL Loss: 16626.996094\n",
      "Train Epoch: 23 [1600/2052 (5%)]\t KLD Loss: 139.638138 \t NLL Loss: 370993.562500\n",
      "Train Epoch: 23 [1760/2052 (5%)]\t KLD Loss: 3.148394 \t NLL Loss: 673.454346\n",
      "Train Epoch: 23 [1920/2052 (6%)]\t KLD Loss: 34.362164 \t NLL Loss: 12977.591797\n",
      "Train Epoch: 24 [160/2052 (0%)]\t KLD Loss: 5.097338 \t NLL Loss: 8069.706055\n",
      "Train Epoch: 24 [320/2052 (1%)]\t KLD Loss: 21.172586 \t NLL Loss: 15722.357422\n",
      "Train Epoch: 24 [480/2052 (1%)]\t KLD Loss: 24.343372 \t NLL Loss: 7372.386719\n",
      "Train Epoch: 24 [640/2052 (2%)]\t KLD Loss: 6.647652 \t NLL Loss: 2363.184814\n",
      "Train Epoch: 24 [800/2052 (2%)]\t KLD Loss: 7.553818 \t NLL Loss: 3491.662354\n",
      "Train Epoch: 24 [960/2052 (3%)]\t KLD Loss: 16.387989 \t NLL Loss: 3594.103760\n",
      "Train Epoch: 24 [1120/2052 (3%)]\t KLD Loss: 29.117762 \t NLL Loss: 65493.035156\n",
      "Train Epoch: 24 [1280/2052 (4%)]\t KLD Loss: 25.577110 \t NLL Loss: 5025.250977\n",
      "Train Epoch: 24 [1440/2052 (4%)]\t KLD Loss: 15.586192 \t NLL Loss: 15557.236328\n",
      "Train Epoch: 24 [1600/2052 (5%)]\t KLD Loss: 108.829605 \t NLL Loss: 365392.000000\n",
      "Train Epoch: 24 [1760/2052 (5%)]\t KLD Loss: 3.592740 \t NLL Loss: 610.395874\n",
      "Train Epoch: 24 [1920/2052 (6%)]\t KLD Loss: 36.025253 \t NLL Loss: 12516.227539\n",
      "Train Epoch: 25 [160/2052 (0%)]\t KLD Loss: 5.456804 \t NLL Loss: 8278.542969\n",
      "Train Epoch: 25 [320/2052 (1%)]\t KLD Loss: 25.639555 \t NLL Loss: 15580.270508\n",
      "Train Epoch: 25 [480/2052 (1%)]\t KLD Loss: 28.270882 \t NLL Loss: 7199.776367\n",
      "Train Epoch: 25 [640/2052 (2%)]\t KLD Loss: 7.046916 \t NLL Loss: 2322.984863\n",
      "Train Epoch: 25 [800/2052 (2%)]\t KLD Loss: 8.513399 \t NLL Loss: 3568.093750\n",
      "Train Epoch: 25 [960/2052 (3%)]\t KLD Loss: 14.092499 \t NLL Loss: 3598.836670\n",
      "Train Epoch: 25 [1120/2052 (3%)]\t KLD Loss: 31.032127 \t NLL Loss: 65008.679688\n",
      "Train Epoch: 25 [1280/2052 (4%)]\t KLD Loss: 24.818317 \t NLL Loss: 4888.661133\n",
      "Train Epoch: 25 [1440/2052 (4%)]\t KLD Loss: 14.930693 \t NLL Loss: 14961.718750\n",
      "Train Epoch: 25 [1600/2052 (5%)]\t KLD Loss: 112.283966 \t NLL Loss: 367939.937500\n",
      "Train Epoch: 25 [1760/2052 (5%)]\t KLD Loss: 3.161234 \t NLL Loss: 589.408936\n",
      "Train Epoch: 25 [1920/2052 (6%)]\t KLD Loss: 33.524048 \t NLL Loss: 12117.027344\n",
      "Train Epoch: 26 [160/2052 (0%)]\t KLD Loss: 4.580558 \t NLL Loss: 8346.074219\n",
      "Train Epoch: 26 [320/2052 (1%)]\t KLD Loss: 24.977386 \t NLL Loss: 15462.769531\n",
      "Train Epoch: 26 [480/2052 (1%)]\t KLD Loss: 28.561749 \t NLL Loss: 7092.328125\n",
      "Train Epoch: 26 [640/2052 (2%)]\t KLD Loss: 7.012549 \t NLL Loss: 2265.264404\n",
      "Train Epoch: 26 [800/2052 (2%)]\t KLD Loss: 7.866911 \t NLL Loss: 3435.759766\n",
      "Train Epoch: 26 [960/2052 (3%)]\t KLD Loss: 21.752090 \t NLL Loss: 4677.821289\n",
      "Train Epoch: 26 [1120/2052 (3%)]\t KLD Loss: 31.252678 \t NLL Loss: 64960.203125\n",
      "Train Epoch: 26 [1280/2052 (4%)]\t KLD Loss: 26.664145 \t NLL Loss: 4614.777344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [1440/2052 (4%)]\t KLD Loss: 16.385029 \t NLL Loss: 14844.986328\n",
      "Train Epoch: 26 [1600/2052 (5%)]\t KLD Loss: 83.828964 \t NLL Loss: 362749.187500\n",
      "Train Epoch: 26 [1760/2052 (5%)]\t KLD Loss: 3.999378 \t NLL Loss: 628.357727\n",
      "Train Epoch: 26 [1920/2052 (6%)]\t KLD Loss: 33.872543 \t NLL Loss: 12467.224609\n",
      "Train Epoch: 27 [160/2052 (0%)]\t KLD Loss: 4.846766 \t NLL Loss: 8477.153320\n",
      "Train Epoch: 27 [320/2052 (1%)]\t KLD Loss: 22.429977 \t NLL Loss: 15778.155273\n",
      "Train Epoch: 27 [480/2052 (1%)]\t KLD Loss: 28.001608 \t NLL Loss: 6936.653320\n",
      "Train Epoch: 27 [640/2052 (2%)]\t KLD Loss: 6.612759 \t NLL Loss: 2251.529541\n",
      "Train Epoch: 27 [800/2052 (2%)]\t KLD Loss: 7.365622 \t NLL Loss: 3456.840332\n",
      "Train Epoch: 27 [960/2052 (3%)]\t KLD Loss: 17.512079 \t NLL Loss: 3924.201904\n",
      "Train Epoch: 27 [1120/2052 (3%)]\t KLD Loss: 29.210350 \t NLL Loss: 64773.496094\n",
      "Train Epoch: 27 [1280/2052 (4%)]\t KLD Loss: 27.671614 \t NLL Loss: 4337.944824\n",
      "Train Epoch: 27 [1440/2052 (4%)]\t KLD Loss: 16.936377 \t NLL Loss: 14533.004883\n",
      "Train Epoch: 27 [1600/2052 (5%)]\t KLD Loss: 79.691750 \t NLL Loss: 360558.125000\n",
      "Train Epoch: 27 [1760/2052 (5%)]\t KLD Loss: 4.005535 \t NLL Loss: 636.092224\n",
      "Train Epoch: 27 [1920/2052 (6%)]\t KLD Loss: 36.658928 \t NLL Loss: 12048.197266\n",
      "Train Epoch: 28 [160/2052 (0%)]\t KLD Loss: 4.773349 \t NLL Loss: 8589.509766\n",
      "Train Epoch: 28 [320/2052 (1%)]\t KLD Loss: 25.532387 \t NLL Loss: 15366.968750\n",
      "Train Epoch: 28 [480/2052 (1%)]\t KLD Loss: 30.759594 \t NLL Loss: 6939.945801\n",
      "Train Epoch: 28 [640/2052 (2%)]\t KLD Loss: 7.628906 \t NLL Loss: 2164.905273\n",
      "Train Epoch: 28 [800/2052 (2%)]\t KLD Loss: 7.857859 \t NLL Loss: 3506.566895\n",
      "Train Epoch: 28 [960/2052 (3%)]\t KLD Loss: 16.124664 \t NLL Loss: 3957.984375\n",
      "Train Epoch: 28 [1120/2052 (3%)]\t KLD Loss: 34.077995 \t NLL Loss: 64460.421875\n",
      "Train Epoch: 28 [1280/2052 (4%)]\t KLD Loss: 28.826374 \t NLL Loss: 4208.850586\n",
      "Train Epoch: 28 [1440/2052 (4%)]\t KLD Loss: 17.589584 \t NLL Loss: 14003.200195\n",
      "Train Epoch: 28 [1600/2052 (5%)]\t KLD Loss: 94.630157 \t NLL Loss: 360390.968750\n",
      "Train Epoch: 28 [1760/2052 (5%)]\t KLD Loss: 3.779622 \t NLL Loss: 552.060913\n",
      "Train Epoch: 28 [1920/2052 (6%)]\t KLD Loss: 35.438576 \t NLL Loss: 11610.909180\n",
      "Train Epoch: 29 [160/2052 (0%)]\t KLD Loss: 4.663383 \t NLL Loss: 8672.781250\n",
      "Train Epoch: 29 [320/2052 (1%)]\t KLD Loss: 24.306753 \t NLL Loss: 15315.792969\n",
      "Train Epoch: 29 [480/2052 (1%)]\t KLD Loss: 29.516268 \t NLL Loss: 7026.473145\n",
      "Train Epoch: 29 [640/2052 (2%)]\t KLD Loss: 6.817321 \t NLL Loss: 2125.232178\n",
      "Train Epoch: 29 [800/2052 (2%)]\t KLD Loss: 7.042526 \t NLL Loss: 3413.494141\n",
      "Train Epoch: 29 [960/2052 (3%)]\t KLD Loss: 15.207222 \t NLL Loss: 4164.450195\n",
      "Train Epoch: 29 [1120/2052 (3%)]\t KLD Loss: 27.012600 \t NLL Loss: 64895.585938\n",
      "Train Epoch: 29 [1280/2052 (4%)]\t KLD Loss: 24.707445 \t NLL Loss: 4273.810547\n",
      "Train Epoch: 29 [1440/2052 (4%)]\t KLD Loss: 16.913406 \t NLL Loss: 13298.000000\n",
      "Train Epoch: 29 [1600/2052 (5%)]\t KLD Loss: 85.218102 \t NLL Loss: 359942.593750\n",
      "Train Epoch: 29 [1760/2052 (5%)]\t KLD Loss: 4.195617 \t NLL Loss: 570.706299\n",
      "Train Epoch: 29 [1920/2052 (6%)]\t KLD Loss: 37.400551 \t NLL Loss: 11418.632812\n",
      "Train Epoch: 30 [160/2052 (0%)]\t KLD Loss: 5.181957 \t NLL Loss: 8668.175781\n",
      "Train Epoch: 30 [320/2052 (1%)]\t KLD Loss: 27.520039 \t NLL Loss: 15344.015625\n",
      "Train Epoch: 30 [480/2052 (1%)]\t KLD Loss: 32.085907 \t NLL Loss: 6806.905273\n",
      "Train Epoch: 30 [640/2052 (2%)]\t KLD Loss: 6.715064 \t NLL Loss: 2064.400879\n",
      "Train Epoch: 30 [800/2052 (2%)]\t KLD Loss: 6.822975 \t NLL Loss: 3451.996582\n",
      "Train Epoch: 30 [960/2052 (3%)]\t KLD Loss: 11.473957 \t NLL Loss: 3612.092529\n",
      "Train Epoch: 30 [1120/2052 (3%)]\t KLD Loss: 31.887638 \t NLL Loss: 64390.589844\n",
      "Train Epoch: 30 [1280/2052 (4%)]\t KLD Loss: 28.079063 \t NLL Loss: 3806.454590\n",
      "Train Epoch: 30 [1440/2052 (4%)]\t KLD Loss: 14.617167 \t NLL Loss: 14124.001953\n",
      "Train Epoch: 30 [1600/2052 (5%)]\t KLD Loss: 104.519745 \t NLL Loss: 361162.843750\n",
      "Train Epoch: 30 [1760/2052 (5%)]\t KLD Loss: 4.449945 \t NLL Loss: 446.312134\n",
      "Train Epoch: 30 [1920/2052 (6%)]\t KLD Loss: 35.654751 \t NLL Loss: 10936.777344\n",
      "Train Epoch: 31 [160/2052 (0%)]\t KLD Loss: 5.323423 \t NLL Loss: 8499.043945\n",
      "Train Epoch: 31 [320/2052 (1%)]\t KLD Loss: 26.689873 \t NLL Loss: 15114.625000\n",
      "Train Epoch: 31 [480/2052 (1%)]\t KLD Loss: 32.427429 \t NLL Loss: 6857.888672\n",
      "Train Epoch: 31 [640/2052 (2%)]\t KLD Loss: 7.664149 \t NLL Loss: 1927.177979\n",
      "Train Epoch: 31 [800/2052 (2%)]\t KLD Loss: 7.554502 \t NLL Loss: 3448.459473\n",
      "Train Epoch: 31 [960/2052 (3%)]\t KLD Loss: 11.427597 \t NLL Loss: 3630.422852\n",
      "Train Epoch: 31 [1120/2052 (3%)]\t KLD Loss: 35.087936 \t NLL Loss: 63971.949219\n",
      "Train Epoch: 31 [1280/2052 (4%)]\t KLD Loss: 30.818623 \t NLL Loss: 3739.818848\n",
      "Train Epoch: 31 [1440/2052 (4%)]\t KLD Loss: 17.389185 \t NLL Loss: 12721.886719\n",
      "Train Epoch: 31 [1600/2052 (5%)]\t KLD Loss: 138.785080 \t NLL Loss: 361038.562500\n",
      "Train Epoch: 31 [1760/2052 (5%)]\t KLD Loss: 4.432711 \t NLL Loss: 441.860901\n",
      "Train Epoch: 31 [1920/2052 (6%)]\t KLD Loss: 39.885880 \t NLL Loss: 10351.048828\n",
      "Train Epoch: 32 [160/2052 (0%)]\t KLD Loss: 6.410324 \t NLL Loss: 8423.137695\n",
      "Train Epoch: 32 [320/2052 (1%)]\t KLD Loss: 34.037010 \t NLL Loss: 14769.796875\n",
      "Train Epoch: 32 [480/2052 (1%)]\t KLD Loss: 38.272919 \t NLL Loss: 6780.406250\n",
      "Train Epoch: 32 [640/2052 (2%)]\t KLD Loss: 8.148103 \t NLL Loss: 1880.258911\n",
      "Train Epoch: 32 [800/2052 (2%)]\t KLD Loss: 7.511561 \t NLL Loss: 3346.485352\n",
      "Train Epoch: 32 [960/2052 (3%)]\t KLD Loss: 10.015520 \t NLL Loss: 3652.137451\n",
      "Train Epoch: 32 [1120/2052 (3%)]\t KLD Loss: 29.954956 \t NLL Loss: 63571.187500\n",
      "Train Epoch: 32 [1280/2052 (4%)]\t KLD Loss: 29.851685 \t NLL Loss: 5098.479492\n",
      "Train Epoch: 32 [1440/2052 (4%)]\t KLD Loss: 18.088535 \t NLL Loss: 11936.599609\n",
      "Train Epoch: 32 [1600/2052 (5%)]\t KLD Loss: 111.041420 \t NLL Loss: 357534.812500\n",
      "Train Epoch: 32 [1760/2052 (5%)]\t KLD Loss: 4.319290 \t NLL Loss: 434.378204\n",
      "Train Epoch: 32 [1920/2052 (6%)]\t KLD Loss: 36.613144 \t NLL Loss: 9918.597656\n",
      "Train Epoch: 33 [160/2052 (0%)]\t KLD Loss: 5.470089 \t NLL Loss: 8274.463867\n",
      "Train Epoch: 33 [320/2052 (1%)]\t KLD Loss: 33.302902 \t NLL Loss: 15218.551758\n",
      "Train Epoch: 33 [480/2052 (1%)]\t KLD Loss: 36.810631 \t NLL Loss: 6741.416016\n",
      "Train Epoch: 33 [640/2052 (2%)]\t KLD Loss: 7.855681 \t NLL Loss: 1888.761475\n",
      "Train Epoch: 33 [800/2052 (2%)]\t KLD Loss: 8.110946 \t NLL Loss: 3333.958740\n",
      "Train Epoch: 33 [960/2052 (3%)]\t KLD Loss: 9.332191 \t NLL Loss: 4223.032715\n",
      "Train Epoch: 33 [1120/2052 (3%)]\t KLD Loss: 33.451416 \t NLL Loss: 63743.375000\n",
      "Train Epoch: 33 [1280/2052 (4%)]\t KLD Loss: 29.815556 \t NLL Loss: 3753.894531\n",
      "Train Epoch: 33 [1440/2052 (4%)]\t KLD Loss: 20.201933 \t NLL Loss: 11109.767578\n",
      "Train Epoch: 33 [1600/2052 (5%)]\t KLD Loss: 131.975662 \t NLL Loss: 357051.187500\n",
      "Train Epoch: 33 [1760/2052 (5%)]\t KLD Loss: 4.012989 \t NLL Loss: 463.582916\n",
      "Train Epoch: 33 [1920/2052 (6%)]\t KLD Loss: 36.892365 \t NLL Loss: 9686.211914\n",
      "Train Epoch: 34 [160/2052 (0%)]\t KLD Loss: 5.823923 \t NLL Loss: 7742.609375\n",
      "Train Epoch: 34 [320/2052 (1%)]\t KLD Loss: 31.630293 \t NLL Loss: 14873.715820\n",
      "Train Epoch: 34 [480/2052 (1%)]\t KLD Loss: 37.057423 \t NLL Loss: 6641.540527\n",
      "Train Epoch: 34 [640/2052 (2%)]\t KLD Loss: 8.133800 \t NLL Loss: 1854.825562\n",
      "Train Epoch: 34 [800/2052 (2%)]\t KLD Loss: 8.245272 \t NLL Loss: 3456.382812\n",
      "Train Epoch: 34 [960/2052 (3%)]\t KLD Loss: 9.500492 \t NLL Loss: 3517.991699\n",
      "Train Epoch: 34 [1120/2052 (3%)]\t KLD Loss: 36.519569 \t NLL Loss: 63492.539062\n",
      "Train Epoch: 34 [1280/2052 (4%)]\t KLD Loss: 31.862568 \t NLL Loss: 3838.341309\n",
      "Train Epoch: 34 [1440/2052 (4%)]\t KLD Loss: 23.820171 \t NLL Loss: 11109.322266\n",
      "Train Epoch: 34 [1600/2052 (5%)]\t KLD Loss: 175.688507 \t NLL Loss: 365275.562500\n",
      "Train Epoch: 34 [1760/2052 (5%)]\t KLD Loss: 4.418830 \t NLL Loss: 455.033203\n",
      "Train Epoch: 34 [1920/2052 (6%)]\t KLD Loss: 37.122723 \t NLL Loss: 10111.496094\n",
      "Train Epoch: 35 [160/2052 (0%)]\t KLD Loss: 8.002701 \t NLL Loss: 7017.125000\n",
      "Train Epoch: 35 [320/2052 (1%)]\t KLD Loss: 38.906517 \t NLL Loss: 14871.903320\n",
      "Train Epoch: 35 [480/2052 (1%)]\t KLD Loss: 43.544151 \t NLL Loss: 6517.861328\n",
      "Train Epoch: 35 [640/2052 (2%)]\t KLD Loss: 9.100751 \t NLL Loss: 1865.725830\n",
      "Train Epoch: 35 [800/2052 (2%)]\t KLD Loss: 9.266072 \t NLL Loss: 3262.292969\n",
      "Train Epoch: 35 [960/2052 (3%)]\t KLD Loss: 12.643970 \t NLL Loss: 3571.313477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 35 [1120/2052 (3%)]\t KLD Loss: 36.925125 \t NLL Loss: 63025.527344\n",
      "Train Epoch: 35 [1280/2052 (4%)]\t KLD Loss: 30.364231 \t NLL Loss: 3792.725830\n",
      "Train Epoch: 35 [1440/2052 (4%)]\t KLD Loss: 22.629156 \t NLL Loss: 10794.176758\n",
      "Train Epoch: 35 [1600/2052 (5%)]\t KLD Loss: 104.656624 \t NLL Loss: 351842.468750\n",
      "Train Epoch: 35 [1760/2052 (5%)]\t KLD Loss: 4.089499 \t NLL Loss: 420.802643\n",
      "Train Epoch: 35 [1920/2052 (6%)]\t KLD Loss: 35.795883 \t NLL Loss: 9922.694336\n",
      "Train Epoch: 36 [160/2052 (0%)]\t KLD Loss: 7.452598 \t NLL Loss: 6837.598633\n",
      "Train Epoch: 36 [320/2052 (1%)]\t KLD Loss: 32.161171 \t NLL Loss: 14706.466797\n",
      "Train Epoch: 36 [480/2052 (1%)]\t KLD Loss: 40.250633 \t NLL Loss: 6670.735840\n",
      "Train Epoch: 36 [640/2052 (2%)]\t KLD Loss: 8.970002 \t NLL Loss: 1871.578613\n",
      "Train Epoch: 36 [800/2052 (2%)]\t KLD Loss: 9.125174 \t NLL Loss: 3243.593018\n",
      "Train Epoch: 36 [960/2052 (3%)]\t KLD Loss: 14.675346 \t NLL Loss: 3176.154297\n",
      "Train Epoch: 36 [1120/2052 (3%)]\t KLD Loss: 36.472008 \t NLL Loss: 63119.046875\n",
      "Train Epoch: 36 [1280/2052 (4%)]\t KLD Loss: 27.853962 \t NLL Loss: 3922.861572\n",
      "Train Epoch: 36 [1440/2052 (4%)]\t KLD Loss: 23.897045 \t NLL Loss: 10505.310547\n",
      "Train Epoch: 36 [1600/2052 (5%)]\t KLD Loss: 172.433807 \t NLL Loss: 340735.000000\n",
      "Train Epoch: 36 [1760/2052 (5%)]\t KLD Loss: 3.596691 \t NLL Loss: 427.723389\n",
      "Train Epoch: 36 [1920/2052 (6%)]\t KLD Loss: 34.829727 \t NLL Loss: 9580.898438\n",
      "Train Epoch: 37 [160/2052 (0%)]\t KLD Loss: 7.822430 \t NLL Loss: 6022.814453\n",
      "Train Epoch: 37 [320/2052 (1%)]\t KLD Loss: 29.526348 \t NLL Loss: 14557.863281\n",
      "Train Epoch: 37 [480/2052 (1%)]\t KLD Loss: 37.122494 \t NLL Loss: 6576.013184\n",
      "Train Epoch: 37 [640/2052 (2%)]\t KLD Loss: 7.979915 \t NLL Loss: 1819.964111\n",
      "Train Epoch: 37 [800/2052 (2%)]\t KLD Loss: 7.583687 \t NLL Loss: 3344.243408\n",
      "Train Epoch: 37 [960/2052 (3%)]\t KLD Loss: 12.924176 \t NLL Loss: 3033.010498\n",
      "Train Epoch: 37 [1120/2052 (3%)]\t KLD Loss: 34.192406 \t NLL Loss: 63069.398438\n",
      "Train Epoch: 37 [1280/2052 (4%)]\t KLD Loss: 26.441448 \t NLL Loss: 3836.985352\n",
      "Train Epoch: 37 [1440/2052 (4%)]\t KLD Loss: 21.753677 \t NLL Loss: 10450.546875\n",
      "Train Epoch: 37 [1600/2052 (5%)]\t KLD Loss: 157.313217 \t NLL Loss: 327473.718750\n",
      "Train Epoch: 37 [1760/2052 (5%)]\t KLD Loss: 3.924282 \t NLL Loss: 416.302063\n",
      "Train Epoch: 37 [1920/2052 (6%)]\t KLD Loss: 34.271980 \t NLL Loss: 10140.318359\n",
      "Train Epoch: 38 [160/2052 (0%)]\t KLD Loss: 10.156581 \t NLL Loss: 5472.005371\n",
      "Train Epoch: 38 [320/2052 (1%)]\t KLD Loss: 33.805733 \t NLL Loss: 14042.898438\n",
      "Train Epoch: 38 [480/2052 (1%)]\t KLD Loss: 36.049686 \t NLL Loss: 6499.701172\n",
      "Train Epoch: 38 [640/2052 (2%)]\t KLD Loss: 7.568017 \t NLL Loss: 1874.568481\n",
      "Train Epoch: 38 [800/2052 (2%)]\t KLD Loss: 8.112448 \t NLL Loss: 3230.993652\n",
      "Train Epoch: 38 [960/2052 (3%)]\t KLD Loss: 16.040272 \t NLL Loss: 3856.820801\n",
      "Train Epoch: 38 [1120/2052 (3%)]\t KLD Loss: 38.351856 \t NLL Loss: 63898.261719\n",
      "Train Epoch: 38 [1280/2052 (4%)]\t KLD Loss: 26.701784 \t NLL Loss: 3786.779785\n",
      "Train Epoch: 38 [1440/2052 (4%)]\t KLD Loss: 19.808788 \t NLL Loss: 10069.593750\n",
      "Train Epoch: 38 [1600/2052 (5%)]\t KLD Loss: 169.146957 \t NLL Loss: 304181.843750\n",
      "Train Epoch: 38 [1760/2052 (5%)]\t KLD Loss: 4.017906 \t NLL Loss: 400.189972\n",
      "Train Epoch: 38 [1920/2052 (6%)]\t KLD Loss: 33.399406 \t NLL Loss: 10095.394531\n",
      "Train Epoch: 39 [160/2052 (0%)]\t KLD Loss: 9.036607 \t NLL Loss: 5118.423828\n",
      "Train Epoch: 39 [320/2052 (1%)]\t KLD Loss: 35.626183 \t NLL Loss: 14016.031250\n",
      "Train Epoch: 39 [480/2052 (1%)]\t KLD Loss: 40.165192 \t NLL Loss: 6555.119629\n",
      "Train Epoch: 39 [640/2052 (2%)]\t KLD Loss: 7.959409 \t NLL Loss: 1858.561890\n",
      "Train Epoch: 39 [800/2052 (2%)]\t KLD Loss: 7.750655 \t NLL Loss: 3222.564941\n",
      "Train Epoch: 39 [960/2052 (3%)]\t KLD Loss: 10.800760 \t NLL Loss: 3017.507812\n",
      "Train Epoch: 39 [1120/2052 (3%)]\t KLD Loss: 30.506321 \t NLL Loss: 63542.375000\n",
      "Train Epoch: 39 [1280/2052 (4%)]\t KLD Loss: 25.386251 \t NLL Loss: 3712.189941\n",
      "Train Epoch: 39 [1440/2052 (4%)]\t KLD Loss: 21.592770 \t NLL Loss: 9553.067383\n",
      "Train Epoch: 39 [1600/2052 (5%)]\t KLD Loss: 289.458771 \t NLL Loss: 277207.718750\n",
      "Train Epoch: 39 [1760/2052 (5%)]\t KLD Loss: 3.794236 \t NLL Loss: 417.745667\n",
      "Train Epoch: 39 [1920/2052 (6%)]\t KLD Loss: 32.479694 \t NLL Loss: 9970.653320\n",
      "Train Epoch: 40 [160/2052 (0%)]\t KLD Loss: 8.928795 \t NLL Loss: 4820.209473\n",
      "Train Epoch: 40 [320/2052 (1%)]\t KLD Loss: 28.318504 \t NLL Loss: 13872.174805\n",
      "Train Epoch: 40 [480/2052 (1%)]\t KLD Loss: 34.503960 \t NLL Loss: 6606.552734\n",
      "Train Epoch: 40 [640/2052 (2%)]\t KLD Loss: 7.593277 \t NLL Loss: 1872.828735\n",
      "Train Epoch: 40 [800/2052 (2%)]\t KLD Loss: 7.995263 \t NLL Loss: 3234.520020\n",
      "Train Epoch: 40 [960/2052 (3%)]\t KLD Loss: 12.227331 \t NLL Loss: 3189.564453\n",
      "Train Epoch: 40 [1120/2052 (3%)]\t KLD Loss: 33.849453 \t NLL Loss: 63991.281250\n",
      "Train Epoch: 40 [1280/2052 (4%)]\t KLD Loss: 28.294035 \t NLL Loss: 3909.401855\n",
      "Train Epoch: 40 [1440/2052 (4%)]\t KLD Loss: 21.030952 \t NLL Loss: 9347.565430\n",
      "Train Epoch: 40 [1600/2052 (5%)]\t KLD Loss: 248.072540 \t NLL Loss: 245098.937500\n",
      "Train Epoch: 40 [1760/2052 (5%)]\t KLD Loss: 4.141886 \t NLL Loss: 430.637756\n",
      "Train Epoch: 40 [1920/2052 (6%)]\t KLD Loss: 38.566971 \t NLL Loss: 9670.201172\n",
      "Train Epoch: 41 [160/2052 (0%)]\t KLD Loss: 9.529230 \t NLL Loss: 4360.938965\n",
      "Train Epoch: 41 [320/2052 (1%)]\t KLD Loss: 32.771721 \t NLL Loss: 13918.191406\n",
      "Train Epoch: 41 [480/2052 (1%)]\t KLD Loss: 37.984600 \t NLL Loss: 6823.864258\n",
      "Train Epoch: 41 [640/2052 (2%)]\t KLD Loss: 7.772042 \t NLL Loss: 1860.829712\n",
      "Train Epoch: 41 [800/2052 (2%)]\t KLD Loss: 7.817186 \t NLL Loss: 3157.570557\n",
      "Train Epoch: 41 [960/2052 (3%)]\t KLD Loss: 13.374584 \t NLL Loss: 2724.488281\n",
      "Train Epoch: 41 [1120/2052 (3%)]\t KLD Loss: 28.849337 \t NLL Loss: 63376.406250\n",
      "Train Epoch: 41 [1280/2052 (4%)]\t KLD Loss: 25.534851 \t NLL Loss: 4043.744141\n",
      "Train Epoch: 41 [1440/2052 (4%)]\t KLD Loss: 21.754681 \t NLL Loss: 9128.257812\n",
      "Train Epoch: 41 [1600/2052 (5%)]\t KLD Loss: 289.684357 \t NLL Loss: 229032.218750\n",
      "Train Epoch: 41 [1760/2052 (5%)]\t KLD Loss: 4.196126 \t NLL Loss: 536.815430\n",
      "Train Epoch: 41 [1920/2052 (6%)]\t KLD Loss: 37.723892 \t NLL Loss: 9151.691406\n",
      "Train Epoch: 42 [160/2052 (0%)]\t KLD Loss: 9.637420 \t NLL Loss: 4273.617676\n",
      "Train Epoch: 42 [320/2052 (1%)]\t KLD Loss: 34.781960 \t NLL Loss: 13732.873047\n",
      "Train Epoch: 42 [480/2052 (1%)]\t KLD Loss: 40.237820 \t NLL Loss: 6755.685059\n",
      "Train Epoch: 42 [640/2052 (2%)]\t KLD Loss: 8.589345 \t NLL Loss: 1858.192871\n",
      "Train Epoch: 42 [800/2052 (2%)]\t KLD Loss: 8.502964 \t NLL Loss: 3180.978516\n",
      "Train Epoch: 42 [960/2052 (3%)]\t KLD Loss: 9.832607 \t NLL Loss: 3280.658447\n",
      "Train Epoch: 42 [1120/2052 (3%)]\t KLD Loss: 35.909164 \t NLL Loss: 64419.773438\n",
      "Train Epoch: 42 [1280/2052 (4%)]\t KLD Loss: 26.382629 \t NLL Loss: 3788.290527\n",
      "Train Epoch: 42 [1440/2052 (4%)]\t KLD Loss: 21.025482 \t NLL Loss: 9484.431641\n",
      "Train Epoch: 42 [1600/2052 (5%)]\t KLD Loss: 331.919220 \t NLL Loss: 192332.328125\n",
      "Train Epoch: 42 [1760/2052 (5%)]\t KLD Loss: 3.977177 \t NLL Loss: 410.361511\n",
      "Train Epoch: 42 [1920/2052 (6%)]\t KLD Loss: 34.404526 \t NLL Loss: 9234.821289\n",
      "Train Epoch: 43 [160/2052 (0%)]\t KLD Loss: 9.943778 \t NLL Loss: 4028.519531\n",
      "Train Epoch: 43 [320/2052 (1%)]\t KLD Loss: 33.242722 \t NLL Loss: 13458.757812\n",
      "Train Epoch: 43 [480/2052 (1%)]\t KLD Loss: 38.123070 \t NLL Loss: 6788.992676\n",
      "Train Epoch: 43 [640/2052 (2%)]\t KLD Loss: 7.793822 \t NLL Loss: 1890.601685\n",
      "Train Epoch: 43 [800/2052 (2%)]\t KLD Loss: 8.518178 \t NLL Loss: 3310.199463\n",
      "Train Epoch: 43 [960/2052 (3%)]\t KLD Loss: 10.959012 \t NLL Loss: 2866.243408\n",
      "Train Epoch: 43 [1120/2052 (3%)]\t KLD Loss: 30.762646 \t NLL Loss: 63324.695312\n",
      "Train Epoch: 43 [1280/2052 (4%)]\t KLD Loss: 27.181641 \t NLL Loss: 4124.666016\n",
      "Train Epoch: 43 [1440/2052 (4%)]\t KLD Loss: 19.856253 \t NLL Loss: 10067.103516\n",
      "Train Epoch: 43 [1600/2052 (5%)]\t KLD Loss: 416.740540 \t NLL Loss: 170739.375000\n",
      "Train Epoch: 43 [1760/2052 (5%)]\t KLD Loss: 3.985231 \t NLL Loss: 414.648895\n",
      "Train Epoch: 43 [1920/2052 (6%)]\t KLD Loss: 34.810158 \t NLL Loss: 9058.529297\n",
      "Train Epoch: 44 [160/2052 (0%)]\t KLD Loss: 8.941216 \t NLL Loss: 4263.462891\n",
      "Train Epoch: 44 [320/2052 (1%)]\t KLD Loss: 32.753719 \t NLL Loss: 13197.206055\n",
      "Train Epoch: 44 [480/2052 (1%)]\t KLD Loss: 37.123520 \t NLL Loss: 6679.832031\n",
      "Train Epoch: 44 [640/2052 (2%)]\t KLD Loss: 7.489748 \t NLL Loss: 1895.727051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 44 [800/2052 (2%)]\t KLD Loss: 7.901770 \t NLL Loss: 3197.725830\n",
      "Train Epoch: 44 [960/2052 (3%)]\t KLD Loss: 12.445255 \t NLL Loss: 3069.494629\n",
      "Train Epoch: 44 [1120/2052 (3%)]\t KLD Loss: 28.104622 \t NLL Loss: 63233.902344\n",
      "Train Epoch: 44 [1280/2052 (4%)]\t KLD Loss: 27.168379 \t NLL Loss: 3776.101562\n",
      "Train Epoch: 44 [1440/2052 (4%)]\t KLD Loss: 19.841524 \t NLL Loss: 9220.857422\n",
      "Train Epoch: 44 [1600/2052 (5%)]\t KLD Loss: 301.617737 \t NLL Loss: 168677.484375\n",
      "Train Epoch: 44 [1760/2052 (5%)]\t KLD Loss: 4.345715 \t NLL Loss: 389.495239\n",
      "Train Epoch: 44 [1920/2052 (6%)]\t KLD Loss: 40.681141 \t NLL Loss: 8796.588867\n",
      "Train Epoch: 45 [160/2052 (0%)]\t KLD Loss: 10.385862 \t NLL Loss: 3604.601074\n",
      "Train Epoch: 45 [320/2052 (1%)]\t KLD Loss: 30.893787 \t NLL Loss: 13086.639648\n",
      "Train Epoch: 45 [480/2052 (1%)]\t KLD Loss: 36.930798 \t NLL Loss: 6813.867188\n",
      "Train Epoch: 45 [640/2052 (2%)]\t KLD Loss: 8.169844 \t NLL Loss: 1859.689453\n",
      "Train Epoch: 45 [800/2052 (2%)]\t KLD Loss: 8.521316 \t NLL Loss: 3327.367188\n",
      "Train Epoch: 45 [960/2052 (3%)]\t KLD Loss: 10.185152 \t NLL Loss: 2972.112549\n",
      "Train Epoch: 45 [1120/2052 (3%)]\t KLD Loss: 30.366156 \t NLL Loss: 63059.675781\n",
      "Train Epoch: 45 [1280/2052 (4%)]\t KLD Loss: 25.491909 \t NLL Loss: 3803.523926\n",
      "Train Epoch: 45 [1440/2052 (4%)]\t KLD Loss: 18.297867 \t NLL Loss: 9697.845703\n",
      "Train Epoch: 45 [1600/2052 (5%)]\t KLD Loss: 445.519226 \t NLL Loss: 162190.468750\n",
      "Train Epoch: 45 [1760/2052 (5%)]\t KLD Loss: 3.807210 \t NLL Loss: 399.579803\n",
      "Train Epoch: 45 [1920/2052 (6%)]\t KLD Loss: 32.918610 \t NLL Loss: 8694.041016\n",
      "Train Epoch: 46 [160/2052 (0%)]\t KLD Loss: 7.679567 \t NLL Loss: 3823.823242\n",
      "Train Epoch: 46 [320/2052 (1%)]\t KLD Loss: 34.260033 \t NLL Loss: 13394.747070\n",
      "Train Epoch: 46 [480/2052 (1%)]\t KLD Loss: 40.947884 \t NLL Loss: 6696.430664\n",
      "Train Epoch: 46 [640/2052 (2%)]\t KLD Loss: 9.202391 \t NLL Loss: 1900.561768\n",
      "Train Epoch: 46 [800/2052 (2%)]\t KLD Loss: 9.815643 \t NLL Loss: 3255.699951\n",
      "Train Epoch: 46 [960/2052 (3%)]\t KLD Loss: 15.458664 \t NLL Loss: 2911.455811\n",
      "Train Epoch: 46 [1120/2052 (3%)]\t KLD Loss: 32.251106 \t NLL Loss: 63481.414062\n",
      "Train Epoch: 46 [1280/2052 (4%)]\t KLD Loss: 26.191580 \t NLL Loss: 3712.890625\n",
      "Train Epoch: 46 [1440/2052 (4%)]\t KLD Loss: 17.230141 \t NLL Loss: 9510.400391\n",
      "Train Epoch: 46 [1600/2052 (5%)]\t KLD Loss: 337.683167 \t NLL Loss: 161253.093750\n",
      "Train Epoch: 46 [1760/2052 (5%)]\t KLD Loss: 3.779341 \t NLL Loss: 442.362854\n",
      "Train Epoch: 46 [1920/2052 (6%)]\t KLD Loss: 35.007786 \t NLL Loss: 8797.710938\n",
      "Train Epoch: 47 [160/2052 (0%)]\t KLD Loss: 9.702511 \t NLL Loss: 3610.593994\n",
      "Train Epoch: 47 [320/2052 (1%)]\t KLD Loss: 27.754459 \t NLL Loss: 13267.282227\n",
      "Train Epoch: 47 [480/2052 (1%)]\t KLD Loss: 34.489822 \t NLL Loss: 6658.932617\n",
      "Train Epoch: 47 [640/2052 (2%)]\t KLD Loss: 8.305066 \t NLL Loss: 1876.591675\n",
      "Train Epoch: 47 [800/2052 (2%)]\t KLD Loss: 9.076263 \t NLL Loss: 3254.335693\n",
      "Train Epoch: 47 [960/2052 (3%)]\t KLD Loss: 12.626580 \t NLL Loss: 2672.570557\n",
      "Train Epoch: 47 [1120/2052 (3%)]\t KLD Loss: 30.230150 \t NLL Loss: 63043.652344\n",
      "Train Epoch: 47 [1280/2052 (4%)]\t KLD Loss: 28.395962 \t NLL Loss: 3539.744385\n",
      "Train Epoch: 47 [1440/2052 (4%)]\t KLD Loss: 18.262825 \t NLL Loss: 8925.375000\n",
      "Train Epoch: 47 [1600/2052 (5%)]\t KLD Loss: 540.005737 \t NLL Loss: 139078.625000\n",
      "Train Epoch: 47 [1760/2052 (5%)]\t KLD Loss: 4.440947 \t NLL Loss: 394.847351\n",
      "Train Epoch: 47 [1920/2052 (6%)]\t KLD Loss: 40.433876 \t NLL Loss: 8521.594727\n",
      "Train Epoch: 48 [160/2052 (0%)]\t KLD Loss: 10.553802 \t NLL Loss: 3336.803467\n",
      "Train Epoch: 48 [320/2052 (1%)]\t KLD Loss: 32.765556 \t NLL Loss: 13186.294922\n",
      "Train Epoch: 48 [480/2052 (1%)]\t KLD Loss: 36.016281 \t NLL Loss: 6646.125000\n",
      "Train Epoch: 48 [640/2052 (2%)]\t KLD Loss: 7.664836 \t NLL Loss: 1856.088013\n",
      "Train Epoch: 48 [800/2052 (2%)]\t KLD Loss: 7.908213 \t NLL Loss: 3171.479004\n",
      "Train Epoch: 48 [960/2052 (3%)]\t KLD Loss: 12.069027 \t NLL Loss: 2348.884033\n",
      "Train Epoch: 48 [1120/2052 (3%)]\t KLD Loss: 32.621689 \t NLL Loss: 63070.261719\n",
      "Train Epoch: 48 [1280/2052 (4%)]\t KLD Loss: 29.377626 \t NLL Loss: 3784.459473\n",
      "Train Epoch: 48 [1440/2052 (4%)]\t KLD Loss: 18.513201 \t NLL Loss: 8394.267578\n",
      "Train Epoch: 48 [1600/2052 (5%)]\t KLD Loss: 492.758759 \t NLL Loss: 124276.882812\n",
      "Train Epoch: 48 [1760/2052 (5%)]\t KLD Loss: 4.605060 \t NLL Loss: 405.321564\n",
      "Train Epoch: 48 [1920/2052 (6%)]\t KLD Loss: 36.769691 \t NLL Loss: 8267.710938\n",
      "Train Epoch: 49 [160/2052 (0%)]\t KLD Loss: 9.377228 \t NLL Loss: 3368.707031\n",
      "Train Epoch: 49 [320/2052 (1%)]\t KLD Loss: 29.650185 \t NLL Loss: 13263.616211\n",
      "Train Epoch: 49 [480/2052 (1%)]\t KLD Loss: 34.849518 \t NLL Loss: 6661.187500\n",
      "Train Epoch: 49 [640/2052 (2%)]\t KLD Loss: 8.409277 \t NLL Loss: 1876.522217\n",
      "Train Epoch: 49 [800/2052 (2%)]\t KLD Loss: 8.783871 \t NLL Loss: 3167.806152\n",
      "Train Epoch: 49 [960/2052 (3%)]\t KLD Loss: 11.773101 \t NLL Loss: 2455.208252\n",
      "Train Epoch: 49 [1120/2052 (3%)]\t KLD Loss: 33.732941 \t NLL Loss: 63043.863281\n",
      "Train Epoch: 49 [1280/2052 (4%)]\t KLD Loss: 28.354975 \t NLL Loss: 3842.111328\n",
      "Train Epoch: 49 [1440/2052 (4%)]\t KLD Loss: 18.849127 \t NLL Loss: 8587.302734\n",
      "Train Epoch: 49 [1600/2052 (5%)]\t KLD Loss: 476.118347 \t NLL Loss: 115735.140625\n",
      "Train Epoch: 49 [1760/2052 (5%)]\t KLD Loss: 4.359995 \t NLL Loss: 484.092010\n",
      "Train Epoch: 49 [1920/2052 (6%)]\t KLD Loss: 42.116234 \t NLL Loss: 8122.305176\n",
      "Train Epoch: 50 [160/2052 (0%)]\t KLD Loss: 10.657151 \t NLL Loss: 3204.732910\n",
      "Train Epoch: 50 [320/2052 (1%)]\t KLD Loss: 33.574921 \t NLL Loss: 12856.535156\n",
      "Train Epoch: 50 [480/2052 (1%)]\t KLD Loss: 38.546494 \t NLL Loss: 6735.781250\n",
      "Train Epoch: 50 [640/2052 (2%)]\t KLD Loss: 8.302588 \t NLL Loss: 1797.290283\n",
      "Train Epoch: 50 [800/2052 (2%)]\t KLD Loss: 7.897172 \t NLL Loss: 3194.143799\n",
      "Train Epoch: 50 [960/2052 (3%)]\t KLD Loss: 10.136702 \t NLL Loss: 2951.212402\n",
      "Train Epoch: 50 [1120/2052 (3%)]\t KLD Loss: 29.912657 \t NLL Loss: 63326.632812\n",
      "Train Epoch: 50 [1280/2052 (4%)]\t KLD Loss: 25.957483 \t NLL Loss: 4032.645996\n",
      "Train Epoch: 50 [1440/2052 (4%)]\t KLD Loss: 18.310909 \t NLL Loss: 8761.133789\n",
      "Train Epoch: 50 [1600/2052 (5%)]\t KLD Loss: 541.558899 \t NLL Loss: 95174.265625\n",
      "Train Epoch: 50 [1760/2052 (5%)]\t KLD Loss: 4.398051 \t NLL Loss: 579.244263\n",
      "Train Epoch: 50 [1920/2052 (6%)]\t KLD Loss: 40.111626 \t NLL Loss: 8170.648438\n",
      "Train Epoch: 51 [160/2052 (0%)]\t KLD Loss: 9.764234 \t NLL Loss: 3115.277344\n",
      "Train Epoch: 51 [320/2052 (1%)]\t KLD Loss: 31.352169 \t NLL Loss: 12670.513672\n",
      "Train Epoch: 51 [480/2052 (1%)]\t KLD Loss: 37.423233 \t NLL Loss: 6554.762695\n",
      "Train Epoch: 51 [640/2052 (2%)]\t KLD Loss: 8.412654 \t NLL Loss: 1822.541992\n",
      "Train Epoch: 51 [800/2052 (2%)]\t KLD Loss: 8.098684 \t NLL Loss: 3196.122559\n",
      "Train Epoch: 51 [960/2052 (3%)]\t KLD Loss: 15.116447 \t NLL Loss: 2607.627686\n",
      "Train Epoch: 51 [1120/2052 (3%)]\t KLD Loss: 31.535515 \t NLL Loss: 62995.582031\n",
      "Train Epoch: 51 [1280/2052 (4%)]\t KLD Loss: 28.701847 \t NLL Loss: 3633.561035\n",
      "Train Epoch: 51 [1440/2052 (4%)]\t KLD Loss: 18.942850 \t NLL Loss: 8675.843750\n",
      "Train Epoch: 51 [1600/2052 (5%)]\t KLD Loss: 466.426636 \t NLL Loss: 93461.953125\n",
      "Train Epoch: 51 [1760/2052 (5%)]\t KLD Loss: 4.405397 \t NLL Loss: 473.523376\n",
      "Train Epoch: 51 [1920/2052 (6%)]\t KLD Loss: 43.446781 \t NLL Loss: 8008.482910\n",
      "Train Epoch: 52 [160/2052 (0%)]\t KLD Loss: 10.337029 \t NLL Loss: 3077.754883\n",
      "Train Epoch: 52 [320/2052 (1%)]\t KLD Loss: 32.695084 \t NLL Loss: 12680.873047\n",
      "Train Epoch: 52 [480/2052 (1%)]\t KLD Loss: 36.892994 \t NLL Loss: 6704.349121\n",
      "Train Epoch: 52 [640/2052 (2%)]\t KLD Loss: 8.037786 \t NLL Loss: 1841.462280\n",
      "Train Epoch: 52 [800/2052 (2%)]\t KLD Loss: 7.908400 \t NLL Loss: 3262.820801\n",
      "Train Epoch: 52 [960/2052 (3%)]\t KLD Loss: 12.687250 \t NLL Loss: 2180.772949\n",
      "Train Epoch: 52 [1120/2052 (3%)]\t KLD Loss: 30.618977 \t NLL Loss: 63061.074219\n",
      "Train Epoch: 52 [1280/2052 (4%)]\t KLD Loss: 26.174791 \t NLL Loss: 3767.168213\n",
      "Train Epoch: 52 [1440/2052 (4%)]\t KLD Loss: 17.466530 \t NLL Loss: 8289.080078\n",
      "Train Epoch: 52 [1600/2052 (5%)]\t KLD Loss: 742.676208 \t NLL Loss: 137460.500000\n",
      "Train Epoch: 52 [1760/2052 (5%)]\t KLD Loss: 4.384230 \t NLL Loss: 508.307251\n",
      "Train Epoch: 52 [1920/2052 (6%)]\t KLD Loss: 43.376556 \t NLL Loss: 7608.401855\n",
      "Train Epoch: 53 [160/2052 (0%)]\t KLD Loss: 11.651699 \t NLL Loss: 2895.994629\n",
      "Train Epoch: 53 [320/2052 (1%)]\t KLD Loss: 33.967026 \t NLL Loss: 12163.230469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 53 [480/2052 (1%)]\t KLD Loss: 37.505432 \t NLL Loss: 6588.820312\n",
      "Train Epoch: 53 [640/2052 (2%)]\t KLD Loss: 8.191256 \t NLL Loss: 1845.168823\n",
      "Train Epoch: 53 [800/2052 (2%)]\t KLD Loss: 8.816065 \t NLL Loss: 3101.094238\n",
      "Train Epoch: 53 [960/2052 (3%)]\t KLD Loss: 11.843338 \t NLL Loss: 1879.695557\n",
      "Train Epoch: 53 [1120/2052 (3%)]\t KLD Loss: 32.007763 \t NLL Loss: 63342.945312\n",
      "Train Epoch: 53 [1280/2052 (4%)]\t KLD Loss: 24.068085 \t NLL Loss: 3565.496094\n",
      "Train Epoch: 53 [1440/2052 (4%)]\t KLD Loss: 17.783718 \t NLL Loss: 7845.586426\n",
      "Train Epoch: 53 [1600/2052 (5%)]\t KLD Loss: 445.805145 \t NLL Loss: 79004.773438\n",
      "Train Epoch: 53 [1760/2052 (5%)]\t KLD Loss: 3.978067 \t NLL Loss: 407.987183\n",
      "Train Epoch: 53 [1920/2052 (6%)]\t KLD Loss: 39.119209 \t NLL Loss: 7764.799316\n",
      "Train Epoch: 54 [160/2052 (0%)]\t KLD Loss: 11.790001 \t NLL Loss: 2960.822021\n",
      "Train Epoch: 54 [320/2052 (1%)]\t KLD Loss: 36.859531 \t NLL Loss: 12471.033203\n",
      "Train Epoch: 54 [480/2052 (1%)]\t KLD Loss: 38.163597 \t NLL Loss: 6554.651367\n",
      "Train Epoch: 54 [640/2052 (2%)]\t KLD Loss: 8.364709 \t NLL Loss: 1810.215576\n",
      "Train Epoch: 54 [800/2052 (2%)]\t KLD Loss: 9.477360 \t NLL Loss: 3110.547363\n",
      "Train Epoch: 54 [960/2052 (3%)]\t KLD Loss: 11.421924 \t NLL Loss: 2239.682617\n",
      "Train Epoch: 54 [1120/2052 (3%)]\t KLD Loss: 32.600872 \t NLL Loss: 63074.617188\n",
      "Train Epoch: 54 [1280/2052 (4%)]\t KLD Loss: 27.938793 \t NLL Loss: 3723.290039\n",
      "Train Epoch: 54 [1440/2052 (4%)]\t KLD Loss: 18.231400 \t NLL Loss: 7780.433105\n",
      "Train Epoch: 54 [1600/2052 (5%)]\t KLD Loss: 462.970093 \t NLL Loss: 63508.531250\n",
      "Train Epoch: 54 [1760/2052 (5%)]\t KLD Loss: 4.416753 \t NLL Loss: 511.542572\n",
      "Train Epoch: 54 [1920/2052 (6%)]\t KLD Loss: 41.452972 \t NLL Loss: 7804.561035\n",
      "Train Epoch: 55 [160/2052 (0%)]\t KLD Loss: 10.832468 \t NLL Loss: 2801.611816\n",
      "Train Epoch: 55 [320/2052 (1%)]\t KLD Loss: 32.429447 \t NLL Loss: 11928.255859\n",
      "Train Epoch: 55 [480/2052 (1%)]\t KLD Loss: 37.502510 \t NLL Loss: 6619.312500\n",
      "Train Epoch: 55 [640/2052 (2%)]\t KLD Loss: 8.306488 \t NLL Loss: 1798.289795\n",
      "Train Epoch: 55 [800/2052 (2%)]\t KLD Loss: 8.517946 \t NLL Loss: 3126.246094\n",
      "Train Epoch: 55 [960/2052 (3%)]\t KLD Loss: 14.451155 \t NLL Loss: 1825.030518\n",
      "Train Epoch: 55 [1120/2052 (3%)]\t KLD Loss: 34.074043 \t NLL Loss: 62678.906250\n",
      "Train Epoch: 55 [1280/2052 (4%)]\t KLD Loss: 27.821182 \t NLL Loss: 3633.712891\n",
      "Train Epoch: 55 [1440/2052 (4%)]\t KLD Loss: 20.818863 \t NLL Loss: 7756.935547\n",
      "Train Epoch: 55 [1600/2052 (5%)]\t KLD Loss: 569.249023 \t NLL Loss: 48797.761719\n",
      "Train Epoch: 55 [1760/2052 (5%)]\t KLD Loss: 4.313251 \t NLL Loss: 393.536621\n",
      "Train Epoch: 55 [1920/2052 (6%)]\t KLD Loss: 39.612038 \t NLL Loss: 7523.680176\n",
      "Train Epoch: 56 [160/2052 (0%)]\t KLD Loss: 10.304853 \t NLL Loss: 3163.769531\n",
      "Train Epoch: 56 [320/2052 (1%)]\t KLD Loss: 28.841936 \t NLL Loss: 12271.565430\n",
      "Train Epoch: 56 [480/2052 (1%)]\t KLD Loss: 37.401112 \t NLL Loss: 6718.165039\n",
      "Train Epoch: 56 [640/2052 (2%)]\t KLD Loss: 9.289180 \t NLL Loss: 1790.218872\n",
      "Train Epoch: 56 [800/2052 (2%)]\t KLD Loss: 10.216158 \t NLL Loss: 3111.953613\n",
      "Train Epoch: 56 [960/2052 (3%)]\t KLD Loss: 13.931035 \t NLL Loss: 1649.253052\n",
      "Train Epoch: 56 [1120/2052 (3%)]\t KLD Loss: 35.505344 \t NLL Loss: 63138.152344\n",
      "Train Epoch: 56 [1280/2052 (4%)]\t KLD Loss: 31.477541 \t NLL Loss: 3629.081787\n",
      "Train Epoch: 56 [1440/2052 (4%)]\t KLD Loss: 21.953251 \t NLL Loss: 7327.099609\n",
      "Train Epoch: 56 [1600/2052 (5%)]\t KLD Loss: 757.367798 \t NLL Loss: 64358.613281\n",
      "Train Epoch: 56 [1760/2052 (5%)]\t KLD Loss: 4.539674 \t NLL Loss: 428.338776\n",
      "Train Epoch: 56 [1920/2052 (6%)]\t KLD Loss: 41.869507 \t NLL Loss: 7548.288086\n",
      "Train Epoch: 57 [160/2052 (0%)]\t KLD Loss: 10.784700 \t NLL Loss: 3848.330811\n",
      "Train Epoch: 57 [320/2052 (1%)]\t KLD Loss: 34.056305 \t NLL Loss: 11610.431641\n",
      "Train Epoch: 57 [480/2052 (1%)]\t KLD Loss: 38.334007 \t NLL Loss: 6606.413574\n",
      "Train Epoch: 57 [640/2052 (2%)]\t KLD Loss: 7.968000 \t NLL Loss: 1847.199829\n",
      "Train Epoch: 57 [800/2052 (2%)]\t KLD Loss: 8.381079 \t NLL Loss: 3069.284180\n",
      "Train Epoch: 57 [960/2052 (3%)]\t KLD Loss: 12.586274 \t NLL Loss: 2063.208984\n",
      "Train Epoch: 57 [1120/2052 (3%)]\t KLD Loss: 28.599583 \t NLL Loss: 63554.582031\n",
      "Train Epoch: 57 [1280/2052 (4%)]\t KLD Loss: 24.760891 \t NLL Loss: 3685.914062\n",
      "Train Epoch: 57 [1440/2052 (4%)]\t KLD Loss: 19.913712 \t NLL Loss: 7511.252930\n",
      "Train Epoch: 57 [1600/2052 (5%)]\t KLD Loss: 468.472198 \t NLL Loss: 54883.527344\n",
      "Train Epoch: 57 [1760/2052 (5%)]\t KLD Loss: 4.051672 \t NLL Loss: 414.727264\n",
      "Train Epoch: 57 [1920/2052 (6%)]\t KLD Loss: 45.273540 \t NLL Loss: 7303.618164\n",
      "Train Epoch: 58 [160/2052 (0%)]\t KLD Loss: 10.541298 \t NLL Loss: 2760.501953\n",
      "Train Epoch: 58 [320/2052 (1%)]\t KLD Loss: 30.951756 \t NLL Loss: 11174.902344\n",
      "Train Epoch: 58 [480/2052 (1%)]\t KLD Loss: 34.574741 \t NLL Loss: 6492.565430\n",
      "Train Epoch: 58 [640/2052 (2%)]\t KLD Loss: 8.071897 \t NLL Loss: 1837.863770\n",
      "Train Epoch: 58 [800/2052 (2%)]\t KLD Loss: 8.439423 \t NLL Loss: 3033.348633\n",
      "Train Epoch: 58 [960/2052 (3%)]\t KLD Loss: 11.357785 \t NLL Loss: 1794.297852\n",
      "Train Epoch: 58 [1120/2052 (3%)]\t KLD Loss: 33.062393 \t NLL Loss: 63680.843750\n",
      "Train Epoch: 58 [1280/2052 (4%)]\t KLD Loss: 30.315496 \t NLL Loss: 4004.020264\n",
      "Train Epoch: 58 [1440/2052 (4%)]\t KLD Loss: 22.208725 \t NLL Loss: 7776.508789\n",
      "Train Epoch: 58 [1600/2052 (5%)]\t KLD Loss: 701.227661 \t NLL Loss: 47330.902344\n",
      "Train Epoch: 58 [1760/2052 (5%)]\t KLD Loss: 4.336811 \t NLL Loss: 404.596527\n",
      "Train Epoch: 58 [1920/2052 (6%)]\t KLD Loss: 39.818916 \t NLL Loss: 7450.518555\n",
      "Train Epoch: 59 [160/2052 (0%)]\t KLD Loss: 10.560186 \t NLL Loss: 2808.569824\n",
      "Train Epoch: 59 [320/2052 (1%)]\t KLD Loss: 32.476688 \t NLL Loss: 11406.542969\n",
      "Train Epoch: 59 [480/2052 (1%)]\t KLD Loss: 37.666321 \t NLL Loss: 6373.434570\n",
      "Train Epoch: 59 [640/2052 (2%)]\t KLD Loss: 8.526644 \t NLL Loss: 1810.402100\n",
      "Train Epoch: 59 [800/2052 (2%)]\t KLD Loss: 10.018320 \t NLL Loss: 3008.019287\n",
      "Train Epoch: 59 [960/2052 (3%)]\t KLD Loss: 14.685983 \t NLL Loss: 1572.204834\n",
      "Train Epoch: 59 [1120/2052 (3%)]\t KLD Loss: 32.386066 \t NLL Loss: 63230.437500\n",
      "Train Epoch: 59 [1280/2052 (4%)]\t KLD Loss: 29.543285 \t NLL Loss: 3920.544922\n",
      "Train Epoch: 59 [1440/2052 (4%)]\t KLD Loss: 21.092617 \t NLL Loss: 7408.687988\n",
      "Train Epoch: 59 [1600/2052 (5%)]\t KLD Loss: 566.337830 \t NLL Loss: 57324.886719\n",
      "Train Epoch: 59 [1760/2052 (5%)]\t KLD Loss: 4.650227 \t NLL Loss: 378.850433\n",
      "Train Epoch: 59 [1920/2052 (6%)]\t KLD Loss: 40.768738 \t NLL Loss: 7266.989258\n",
      "Train Epoch: 60 [160/2052 (0%)]\t KLD Loss: 11.794684 \t NLL Loss: 2717.586426\n",
      "Train Epoch: 60 [320/2052 (1%)]\t KLD Loss: 34.538368 \t NLL Loss: 11158.455078\n",
      "Train Epoch: 60 [480/2052 (1%)]\t KLD Loss: 38.519119 \t NLL Loss: 6335.417969\n",
      "Train Epoch: 60 [640/2052 (2%)]\t KLD Loss: 8.321312 \t NLL Loss: 1857.347412\n",
      "Train Epoch: 60 [800/2052 (2%)]\t KLD Loss: 8.862885 \t NLL Loss: 2961.922607\n",
      "Train Epoch: 60 [960/2052 (3%)]\t KLD Loss: 10.918871 \t NLL Loss: 1771.781738\n",
      "Train Epoch: 60 [1120/2052 (3%)]\t KLD Loss: 28.261604 \t NLL Loss: 63531.226562\n",
      "Train Epoch: 60 [1280/2052 (4%)]\t KLD Loss: 25.792589 \t NLL Loss: 3888.851074\n",
      "Train Epoch: 60 [1440/2052 (4%)]\t KLD Loss: 21.003115 \t NLL Loss: 7026.958984\n",
      "Train Epoch: 60 [1600/2052 (5%)]\t KLD Loss: 450.505341 \t NLL Loss: 66832.125000\n",
      "Train Epoch: 60 [1760/2052 (5%)]\t KLD Loss: 4.661915 \t NLL Loss: 378.598328\n",
      "Train Epoch: 60 [1920/2052 (6%)]\t KLD Loss: 44.790749 \t NLL Loss: 7268.374023\n",
      "Train Epoch: 61 [160/2052 (0%)]\t KLD Loss: 11.229452 \t NLL Loss: 3005.916748\n",
      "Train Epoch: 61 [320/2052 (1%)]\t KLD Loss: 31.939873 \t NLL Loss: 11845.154297\n",
      "Train Epoch: 61 [480/2052 (1%)]\t KLD Loss: 40.423107 \t NLL Loss: 6510.463867\n",
      "Train Epoch: 61 [640/2052 (2%)]\t KLD Loss: 9.075216 \t NLL Loss: 1793.398438\n",
      "Train Epoch: 61 [800/2052 (2%)]\t KLD Loss: 9.724709 \t NLL Loss: 3039.896973\n",
      "Train Epoch: 61 [960/2052 (3%)]\t KLD Loss: 12.674610 \t NLL Loss: 1714.027588\n",
      "Train Epoch: 61 [1120/2052 (3%)]\t KLD Loss: 32.187469 \t NLL Loss: 63699.296875\n",
      "Train Epoch: 61 [1280/2052 (4%)]\t KLD Loss: 27.513775 \t NLL Loss: 4007.460938\n",
      "Train Epoch: 61 [1440/2052 (4%)]\t KLD Loss: 20.050091 \t NLL Loss: 7109.710938\n",
      "Train Epoch: 61 [1600/2052 (5%)]\t KLD Loss: 612.385437 \t NLL Loss: 46518.757812\n",
      "Train Epoch: 61 [1760/2052 (5%)]\t KLD Loss: 4.777225 \t NLL Loss: 399.899506\n",
      "Train Epoch: 61 [1920/2052 (6%)]\t KLD Loss: 44.335136 \t NLL Loss: 7140.084961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 62 [160/2052 (0%)]\t KLD Loss: 11.319088 \t NLL Loss: 2610.590332\n",
      "Train Epoch: 62 [320/2052 (1%)]\t KLD Loss: 31.597019 \t NLL Loss: 11578.381836\n",
      "Train Epoch: 62 [480/2052 (1%)]\t KLD Loss: 37.351822 \t NLL Loss: 6519.444336\n",
      "Train Epoch: 62 [640/2052 (2%)]\t KLD Loss: 8.482655 \t NLL Loss: 1794.461548\n",
      "Train Epoch: 62 [800/2052 (2%)]\t KLD Loss: 9.893488 \t NLL Loss: 2959.339600\n",
      "Train Epoch: 62 [960/2052 (3%)]\t KLD Loss: 12.952244 \t NLL Loss: 1694.398071\n",
      "Train Epoch: 62 [1120/2052 (3%)]\t KLD Loss: 34.114975 \t NLL Loss: 62768.683594\n",
      "Train Epoch: 62 [1280/2052 (4%)]\t KLD Loss: 28.388945 \t NLL Loss: 3992.065186\n",
      "Train Epoch: 62 [1440/2052 (4%)]\t KLD Loss: 23.441710 \t NLL Loss: 9930.858398\n",
      "Train Epoch: 62 [1600/2052 (5%)]\t KLD Loss: 339.401917 \t NLL Loss: 81957.398438\n",
      "Train Epoch: 62 [1760/2052 (5%)]\t KLD Loss: 4.473776 \t NLL Loss: 376.151947\n",
      "Train Epoch: 62 [1920/2052 (6%)]\t KLD Loss: 44.216049 \t NLL Loss: 7072.578125\n",
      "Train Epoch: 63 [160/2052 (0%)]\t KLD Loss: 11.565319 \t NLL Loss: 2511.598145\n",
      "Train Epoch: 63 [320/2052 (1%)]\t KLD Loss: 36.735756 \t NLL Loss: 11610.697266\n",
      "Train Epoch: 63 [480/2052 (1%)]\t KLD Loss: 44.054562 \t NLL Loss: 6530.136719\n",
      "Train Epoch: 63 [640/2052 (2%)]\t KLD Loss: 8.917795 \t NLL Loss: 1793.120117\n",
      "Train Epoch: 63 [800/2052 (2%)]\t KLD Loss: 8.990002 \t NLL Loss: 2979.071777\n",
      "Train Epoch: 63 [960/2052 (3%)]\t KLD Loss: 11.326376 \t NLL Loss: 1465.494385\n",
      "Train Epoch: 63 [1120/2052 (3%)]\t KLD Loss: 30.380522 \t NLL Loss: 62988.257812\n",
      "Train Epoch: 63 [1280/2052 (4%)]\t KLD Loss: 26.010235 \t NLL Loss: 4009.945801\n",
      "Train Epoch: 63 [1440/2052 (4%)]\t KLD Loss: 22.643970 \t NLL Loss: 6970.692383\n",
      "Train Epoch: 63 [1600/2052 (5%)]\t KLD Loss: 579.577820 \t NLL Loss: 42895.960938\n",
      "Train Epoch: 63 [1760/2052 (5%)]\t KLD Loss: 4.504279 \t NLL Loss: 374.707336\n",
      "Train Epoch: 63 [1920/2052 (6%)]\t KLD Loss: 43.069592 \t NLL Loss: 6877.013184\n",
      "Train Epoch: 64 [160/2052 (0%)]\t KLD Loss: 11.081692 \t NLL Loss: 2569.295166\n",
      "Train Epoch: 64 [320/2052 (1%)]\t KLD Loss: 31.728043 \t NLL Loss: 11372.367188\n",
      "Train Epoch: 64 [480/2052 (1%)]\t KLD Loss: 38.828228 \t NLL Loss: 6720.932617\n",
      "Train Epoch: 64 [640/2052 (2%)]\t KLD Loss: 8.221138 \t NLL Loss: 1776.486938\n",
      "Train Epoch: 64 [800/2052 (2%)]\t KLD Loss: 8.723882 \t NLL Loss: 3130.946289\n",
      "Train Epoch: 64 [960/2052 (3%)]\t KLD Loss: 10.951284 \t NLL Loss: 1462.187744\n",
      "Train Epoch: 64 [1120/2052 (3%)]\t KLD Loss: 33.208183 \t NLL Loss: 62887.734375\n",
      "Train Epoch: 64 [1280/2052 (4%)]\t KLD Loss: 28.233335 \t NLL Loss: 4160.911621\n",
      "Train Epoch: 64 [1440/2052 (4%)]\t KLD Loss: 21.611927 \t NLL Loss: 7167.945312\n",
      "Train Epoch: 64 [1600/2052 (5%)]\t KLD Loss: 358.194153 \t NLL Loss: 48851.824219\n",
      "Train Epoch: 64 [1760/2052 (5%)]\t KLD Loss: 4.552232 \t NLL Loss: 450.598633\n",
      "Train Epoch: 64 [1920/2052 (6%)]\t KLD Loss: 44.517635 \t NLL Loss: 7033.163574\n",
      "Train Epoch: 65 [160/2052 (0%)]\t KLD Loss: 10.654232 \t NLL Loss: 3083.013428\n",
      "Train Epoch: 65 [320/2052 (1%)]\t KLD Loss: 31.908344 \t NLL Loss: 11391.246094\n",
      "Train Epoch: 65 [480/2052 (1%)]\t KLD Loss: 36.448727 \t NLL Loss: 6471.861816\n",
      "Train Epoch: 65 [640/2052 (2%)]\t KLD Loss: 7.846421 \t NLL Loss: 1788.019287\n",
      "Train Epoch: 65 [800/2052 (2%)]\t KLD Loss: 7.539707 \t NLL Loss: 3131.371582\n",
      "Train Epoch: 65 [960/2052 (3%)]\t KLD Loss: 8.283007 \t NLL Loss: 2891.572021\n",
      "Train Epoch: 65 [1120/2052 (3%)]\t KLD Loss: 32.794273 \t NLL Loss: 62796.910156\n",
      "Train Epoch: 65 [1280/2052 (4%)]\t KLD Loss: 28.813976 \t NLL Loss: 3578.597168\n",
      "Train Epoch: 65 [1440/2052 (4%)]\t KLD Loss: 21.452440 \t NLL Loss: 6326.122070\n",
      "Train Epoch: 65 [1600/2052 (5%)]\t KLD Loss: 462.627686 \t NLL Loss: 37826.675781\n",
      "Train Epoch: 65 [1760/2052 (5%)]\t KLD Loss: 5.010725 \t NLL Loss: 424.372009\n",
      "Train Epoch: 65 [1920/2052 (6%)]\t KLD Loss: 51.707664 \t NLL Loss: 6885.101562\n",
      "Train Epoch: 66 [160/2052 (0%)]\t KLD Loss: 12.147218 \t NLL Loss: 2648.721191\n",
      "Train Epoch: 66 [320/2052 (1%)]\t KLD Loss: 39.568260 \t NLL Loss: 10835.739258\n",
      "Train Epoch: 66 [480/2052 (1%)]\t KLD Loss: 39.699409 \t NLL Loss: 6369.361328\n",
      "Train Epoch: 66 [640/2052 (2%)]\t KLD Loss: 8.114810 \t NLL Loss: 1814.350708\n",
      "Train Epoch: 66 [800/2052 (2%)]\t KLD Loss: 8.250211 \t NLL Loss: 3050.000488\n",
      "Train Epoch: 66 [960/2052 (3%)]\t KLD Loss: 11.261457 \t NLL Loss: 1513.956177\n",
      "Train Epoch: 66 [1120/2052 (3%)]\t KLD Loss: 30.907143 \t NLL Loss: 62282.492188\n",
      "Train Epoch: 66 [1280/2052 (4%)]\t KLD Loss: 27.501957 \t NLL Loss: 3613.906494\n",
      "Train Epoch: 66 [1440/2052 (4%)]\t KLD Loss: 21.705454 \t NLL Loss: 6146.845215\n",
      "Train Epoch: 66 [1600/2052 (5%)]\t KLD Loss: 462.177856 \t NLL Loss: 27635.646484\n",
      "Train Epoch: 66 [1760/2052 (5%)]\t KLD Loss: 4.509194 \t NLL Loss: 419.892273\n",
      "Train Epoch: 66 [1920/2052 (6%)]\t KLD Loss: 43.933559 \t NLL Loss: 6838.277832\n",
      "Train Epoch: 67 [160/2052 (0%)]\t KLD Loss: 13.121030 \t NLL Loss: 2691.989990\n",
      "Train Epoch: 67 [320/2052 (1%)]\t KLD Loss: 36.540901 \t NLL Loss: 10854.164062\n",
      "Train Epoch: 67 [480/2052 (1%)]\t KLD Loss: 40.298706 \t NLL Loss: 6211.560547\n",
      "Train Epoch: 67 [640/2052 (2%)]\t KLD Loss: 8.746201 \t NLL Loss: 1782.619019\n",
      "Train Epoch: 67 [800/2052 (2%)]\t KLD Loss: 9.658142 \t NLL Loss: 2916.500977\n",
      "Train Epoch: 67 [960/2052 (3%)]\t KLD Loss: 9.776918 \t NLL Loss: 1462.415283\n",
      "Train Epoch: 67 [1120/2052 (3%)]\t KLD Loss: 32.441265 \t NLL Loss: 62444.945312\n",
      "Train Epoch: 67 [1280/2052 (4%)]\t KLD Loss: 26.912991 \t NLL Loss: 3674.868408\n",
      "Train Epoch: 67 [1440/2052 (4%)]\t KLD Loss: 19.405436 \t NLL Loss: 6870.114258\n",
      "Train Epoch: 67 [1600/2052 (5%)]\t KLD Loss: 446.250397 \t NLL Loss: 31881.707031\n",
      "Train Epoch: 67 [1760/2052 (5%)]\t KLD Loss: 4.307544 \t NLL Loss: 372.878601\n",
      "Train Epoch: 67 [1920/2052 (6%)]\t KLD Loss: 44.224224 \t NLL Loss: 6783.357422\n",
      "Train Epoch: 68 [160/2052 (0%)]\t KLD Loss: 10.942901 \t NLL Loss: 2668.958496\n",
      "Train Epoch: 68 [320/2052 (1%)]\t KLD Loss: 38.491982 \t NLL Loss: 10917.330078\n",
      "Train Epoch: 68 [480/2052 (1%)]\t KLD Loss: 42.948898 \t NLL Loss: 6440.025391\n",
      "Train Epoch: 68 [640/2052 (2%)]\t KLD Loss: 8.490647 \t NLL Loss: 1796.941406\n",
      "Train Epoch: 68 [800/2052 (2%)]\t KLD Loss: 9.289774 \t NLL Loss: 3056.877441\n",
      "Train Epoch: 68 [960/2052 (3%)]\t KLD Loss: 9.897285 \t NLL Loss: 1423.509521\n",
      "Train Epoch: 68 [1120/2052 (3%)]\t KLD Loss: 29.594830 \t NLL Loss: 62252.648438\n",
      "Train Epoch: 68 [1280/2052 (4%)]\t KLD Loss: 25.596474 \t NLL Loss: 3698.306885\n",
      "Train Epoch: 68 [1440/2052 (4%)]\t KLD Loss: 20.373203 \t NLL Loss: 7662.356934\n",
      "Train Epoch: 68 [1600/2052 (5%)]\t KLD Loss: 456.267212 \t NLL Loss: 42612.328125\n",
      "Train Epoch: 68 [1760/2052 (5%)]\t KLD Loss: 4.309265 \t NLL Loss: 360.943359\n",
      "Train Epoch: 68 [1920/2052 (6%)]\t KLD Loss: 43.928078 \t NLL Loss: 6697.969727\n",
      "Train Epoch: 69 [160/2052 (0%)]\t KLD Loss: 10.623075 \t NLL Loss: 3402.228027\n",
      "Train Epoch: 69 [320/2052 (1%)]\t KLD Loss: 35.807030 \t NLL Loss: 10438.410156\n",
      "Train Epoch: 69 [480/2052 (1%)]\t KLD Loss: 37.332108 \t NLL Loss: 6279.631836\n",
      "Train Epoch: 69 [640/2052 (2%)]\t KLD Loss: 7.690976 \t NLL Loss: 1795.995850\n",
      "Train Epoch: 69 [800/2052 (2%)]\t KLD Loss: 8.325007 \t NLL Loss: 2921.769531\n",
      "Train Epoch: 69 [960/2052 (3%)]\t KLD Loss: 13.431219 \t NLL Loss: 2770.872803\n",
      "Train Epoch: 69 [1120/2052 (3%)]\t KLD Loss: 28.480974 \t NLL Loss: 62444.308594\n",
      "Train Epoch: 69 [1280/2052 (4%)]\t KLD Loss: 25.477364 \t NLL Loss: 3946.610352\n",
      "Train Epoch: 69 [1440/2052 (4%)]\t KLD Loss: 18.249462 \t NLL Loss: 6565.921875\n",
      "Train Epoch: 69 [1600/2052 (5%)]\t KLD Loss: 441.567444 \t NLL Loss: 27596.269531\n",
      "Train Epoch: 69 [1760/2052 (5%)]\t KLD Loss: 4.967241 \t NLL Loss: 360.521362\n",
      "Train Epoch: 69 [1920/2052 (6%)]\t KLD Loss: 43.904419 \t NLL Loss: 6588.083008\n",
      "Train Epoch: 70 [160/2052 (0%)]\t KLD Loss: 12.865118 \t NLL Loss: 2582.673340\n",
      "Train Epoch: 70 [320/2052 (1%)]\t KLD Loss: 36.564964 \t NLL Loss: 10848.438477\n",
      "Train Epoch: 70 [480/2052 (1%)]\t KLD Loss: 42.017139 \t NLL Loss: 6218.983887\n",
      "Train Epoch: 70 [640/2052 (2%)]\t KLD Loss: 9.018549 \t NLL Loss: 1805.997192\n",
      "Train Epoch: 70 [800/2052 (2%)]\t KLD Loss: 8.960798 \t NLL Loss: 2977.992432\n",
      "Train Epoch: 70 [960/2052 (3%)]\t KLD Loss: 14.476865 \t NLL Loss: 2092.199219\n",
      "Train Epoch: 70 [1120/2052 (3%)]\t KLD Loss: 31.028641 \t NLL Loss: 62550.671875\n",
      "Train Epoch: 70 [1280/2052 (4%)]\t KLD Loss: 26.582916 \t NLL Loss: 4096.585938\n",
      "Train Epoch: 70 [1440/2052 (4%)]\t KLD Loss: 21.197769 \t NLL Loss: 6991.794922\n",
      "Train Epoch: 70 [1600/2052 (5%)]\t KLD Loss: 529.903442 \t NLL Loss: 29767.628906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 70 [1760/2052 (5%)]\t KLD Loss: 5.112403 \t NLL Loss: 380.130127\n",
      "Train Epoch: 70 [1920/2052 (6%)]\t KLD Loss: 46.226707 \t NLL Loss: 6647.913086\n",
      "Train Epoch: 71 [160/2052 (0%)]\t KLD Loss: 13.438473 \t NLL Loss: 2636.244141\n",
      "Train Epoch: 71 [320/2052 (1%)]\t KLD Loss: 33.445293 \t NLL Loss: 10292.262695\n",
      "Train Epoch: 71 [480/2052 (1%)]\t KLD Loss: 37.571846 \t NLL Loss: 6306.333008\n",
      "Train Epoch: 71 [640/2052 (2%)]\t KLD Loss: 8.242517 \t NLL Loss: 1787.404419\n",
      "Train Epoch: 71 [800/2052 (2%)]\t KLD Loss: 7.920286 \t NLL Loss: 2908.583496\n",
      "Train Epoch: 71 [960/2052 (3%)]\t KLD Loss: 14.710974 \t NLL Loss: 2326.281250\n",
      "Train Epoch: 71 [1120/2052 (3%)]\t KLD Loss: 26.016697 \t NLL Loss: 62827.031250\n",
      "Train Epoch: 71 [1280/2052 (4%)]\t KLD Loss: 21.925159 \t NLL Loss: 3700.112305\n",
      "Train Epoch: 71 [1440/2052 (4%)]\t KLD Loss: 18.094181 \t NLL Loss: 7352.445312\n",
      "Train Epoch: 71 [1600/2052 (5%)]\t KLD Loss: 382.663971 \t NLL Loss: 56301.980469\n",
      "Train Epoch: 71 [1760/2052 (5%)]\t KLD Loss: 5.305840 \t NLL Loss: 365.096985\n",
      "Train Epoch: 71 [1920/2052 (6%)]\t KLD Loss: 47.432487 \t NLL Loss: 6652.493164\n",
      "Train Epoch: 72 [160/2052 (0%)]\t KLD Loss: 11.400041 \t NLL Loss: 2767.491211\n",
      "Train Epoch: 72 [320/2052 (1%)]\t KLD Loss: 35.896091 \t NLL Loss: 11116.142578\n",
      "Train Epoch: 72 [480/2052 (1%)]\t KLD Loss: 44.357849 \t NLL Loss: 6238.052734\n",
      "Train Epoch: 72 [640/2052 (2%)]\t KLD Loss: 10.130776 \t NLL Loss: 1784.247070\n",
      "Train Epoch: 72 [800/2052 (2%)]\t KLD Loss: 10.611242 \t NLL Loss: 2951.470947\n",
      "Train Epoch: 72 [960/2052 (3%)]\t KLD Loss: 13.361140 \t NLL Loss: 2056.725830\n",
      "Train Epoch: 72 [1120/2052 (3%)]\t KLD Loss: 33.522400 \t NLL Loss: 62674.363281\n",
      "Train Epoch: 72 [1280/2052 (4%)]\t KLD Loss: 29.787815 \t NLL Loss: 3642.694824\n",
      "Train Epoch: 72 [1440/2052 (4%)]\t KLD Loss: 22.737137 \t NLL Loss: 8058.155762\n",
      "Train Epoch: 72 [1600/2052 (5%)]\t KLD Loss: 489.587433 \t NLL Loss: 26773.640625\n",
      "Train Epoch: 72 [1760/2052 (5%)]\t KLD Loss: 4.680639 \t NLL Loss: 361.904785\n",
      "Train Epoch: 72 [1920/2052 (6%)]\t KLD Loss: 42.876842 \t NLL Loss: 6334.797363\n",
      "Train Epoch: 73 [160/2052 (0%)]\t KLD Loss: 13.556476 \t NLL Loss: 2574.852051\n",
      "Train Epoch: 73 [320/2052 (1%)]\t KLD Loss: 36.629337 \t NLL Loss: 10347.661133\n",
      "Train Epoch: 73 [480/2052 (1%)]\t KLD Loss: 39.132889 \t NLL Loss: 6014.002930\n",
      "Train Epoch: 73 [640/2052 (2%)]\t KLD Loss: 8.982086 \t NLL Loss: 1785.445068\n",
      "Train Epoch: 73 [800/2052 (2%)]\t KLD Loss: 9.822390 \t NLL Loss: 2931.057129\n",
      "Train Epoch: 73 [960/2052 (3%)]\t KLD Loss: 14.050758 \t NLL Loss: 2230.881348\n",
      "Train Epoch: 73 [1120/2052 (3%)]\t KLD Loss: 32.909660 \t NLL Loss: 62760.566406\n",
      "Train Epoch: 73 [1280/2052 (4%)]\t KLD Loss: 26.918585 \t NLL Loss: 3854.125000\n",
      "Train Epoch: 73 [1440/2052 (4%)]\t KLD Loss: 21.159279 \t NLL Loss: 7133.277344\n",
      "Train Epoch: 73 [1600/2052 (5%)]\t KLD Loss: 336.744232 \t NLL Loss: 60276.738281\n",
      "Train Epoch: 73 [1760/2052 (5%)]\t KLD Loss: 4.526335 \t NLL Loss: 423.250153\n",
      "Train Epoch: 73 [1920/2052 (6%)]\t KLD Loss: 40.260887 \t NLL Loss: 6240.077148\n",
      "Train Epoch: 74 [160/2052 (0%)]\t KLD Loss: 12.129181 \t NLL Loss: 2351.882324\n",
      "Train Epoch: 74 [320/2052 (1%)]\t KLD Loss: 36.848423 \t NLL Loss: 10015.575195\n",
      "Train Epoch: 74 [480/2052 (1%)]\t KLD Loss: 41.837044 \t NLL Loss: 5972.161133\n",
      "Train Epoch: 74 [640/2052 (2%)]\t KLD Loss: 9.674451 \t NLL Loss: 1761.194580\n",
      "Train Epoch: 74 [800/2052 (2%)]\t KLD Loss: 10.123888 \t NLL Loss: 2865.941895\n",
      "Train Epoch: 74 [960/2052 (3%)]\t KLD Loss: 12.828815 \t NLL Loss: 1673.034668\n",
      "Train Epoch: 74 [1120/2052 (3%)]\t KLD Loss: 28.765339 \t NLL Loss: 62214.265625\n",
      "Train Epoch: 74 [1280/2052 (4%)]\t KLD Loss: 27.104214 \t NLL Loss: 3796.254395\n",
      "Train Epoch: 74 [1440/2052 (4%)]\t KLD Loss: 19.724504 \t NLL Loss: 6958.838379\n",
      "Train Epoch: 74 [1600/2052 (5%)]\t KLD Loss: 371.530029 \t NLL Loss: 31713.765625\n",
      "Train Epoch: 74 [1760/2052 (5%)]\t KLD Loss: 4.326280 \t NLL Loss: 426.817902\n",
      "Train Epoch: 74 [1920/2052 (6%)]\t KLD Loss: 38.947392 \t NLL Loss: 6238.866699\n",
      "Train Epoch: 75 [160/2052 (0%)]\t KLD Loss: 12.091911 \t NLL Loss: 2455.959961\n",
      "Train Epoch: 75 [320/2052 (1%)]\t KLD Loss: 35.653400 \t NLL Loss: 10099.179688\n",
      "Train Epoch: 75 [480/2052 (1%)]\t KLD Loss: 39.317230 \t NLL Loss: 5962.424805\n",
      "Train Epoch: 75 [640/2052 (2%)]\t KLD Loss: 8.703573 \t NLL Loss: 1772.087036\n",
      "Train Epoch: 75 [800/2052 (2%)]\t KLD Loss: 9.835491 \t NLL Loss: 2862.669434\n",
      "Train Epoch: 75 [960/2052 (3%)]\t KLD Loss: 13.463448 \t NLL Loss: 2890.189453\n",
      "Train Epoch: 75 [1120/2052 (3%)]\t KLD Loss: 29.924957 \t NLL Loss: 61889.769531\n",
      "Train Epoch: 75 [1280/2052 (4%)]\t KLD Loss: 27.589912 \t NLL Loss: 3642.089355\n",
      "Train Epoch: 75 [1440/2052 (4%)]\t KLD Loss: 22.045242 \t NLL Loss: 7105.389648\n",
      "Train Epoch: 75 [1600/2052 (5%)]\t KLD Loss: 508.136139 \t NLL Loss: 39788.296875\n",
      "Train Epoch: 75 [1760/2052 (5%)]\t KLD Loss: 4.911439 \t NLL Loss: 344.534180\n",
      "Train Epoch: 75 [1920/2052 (6%)]\t KLD Loss: 43.785076 \t NLL Loss: 6418.797852\n",
      "Train Epoch: 76 [160/2052 (0%)]\t KLD Loss: 14.747445 \t NLL Loss: 2172.324219\n",
      "Train Epoch: 76 [320/2052 (1%)]\t KLD Loss: 35.224342 \t NLL Loss: 9986.605469\n",
      "Train Epoch: 76 [480/2052 (1%)]\t KLD Loss: 36.306789 \t NLL Loss: 5838.780273\n",
      "Train Epoch: 76 [640/2052 (2%)]\t KLD Loss: 8.809479 \t NLL Loss: 1791.921143\n",
      "Train Epoch: 76 [800/2052 (2%)]\t KLD Loss: 9.952999 \t NLL Loss: 2908.859131\n",
      "Train Epoch: 76 [960/2052 (3%)]\t KLD Loss: 12.732800 \t NLL Loss: 1776.694824\n",
      "Train Epoch: 76 [1120/2052 (3%)]\t KLD Loss: 30.722317 \t NLL Loss: 61935.144531\n",
      "Train Epoch: 76 [1280/2052 (4%)]\t KLD Loss: 27.380157 \t NLL Loss: 3584.462891\n",
      "Train Epoch: 76 [1440/2052 (4%)]\t KLD Loss: 22.860086 \t NLL Loss: 7109.641113\n",
      "Train Epoch: 76 [1600/2052 (5%)]\t KLD Loss: 405.282837 \t NLL Loss: 38635.757812\n",
      "Train Epoch: 76 [1760/2052 (5%)]\t KLD Loss: 4.985900 \t NLL Loss: 335.311066\n",
      "Train Epoch: 76 [1920/2052 (6%)]\t KLD Loss: 43.080757 \t NLL Loss: 6333.949219\n",
      "Train Epoch: 77 [160/2052 (0%)]\t KLD Loss: 10.870214 \t NLL Loss: 2545.164795\n",
      "Train Epoch: 77 [320/2052 (1%)]\t KLD Loss: 32.195923 \t NLL Loss: 10103.435547\n",
      "Train Epoch: 77 [480/2052 (1%)]\t KLD Loss: 38.651325 \t NLL Loss: 5796.488770\n",
      "Train Epoch: 77 [640/2052 (2%)]\t KLD Loss: 8.456010 \t NLL Loss: 1755.836792\n",
      "Train Epoch: 77 [800/2052 (2%)]\t KLD Loss: 8.551203 \t NLL Loss: 2983.815186\n",
      "Train Epoch: 77 [960/2052 (3%)]\t KLD Loss: 13.352759 \t NLL Loss: 2346.428711\n",
      "Train Epoch: 77 [1120/2052 (3%)]\t KLD Loss: 31.844795 \t NLL Loss: 61880.328125\n",
      "Train Epoch: 77 [1280/2052 (4%)]\t KLD Loss: 29.563736 \t NLL Loss: 3616.746582\n",
      "Train Epoch: 77 [1440/2052 (4%)]\t KLD Loss: 23.678133 \t NLL Loss: 9683.731445\n",
      "Train Epoch: 77 [1600/2052 (5%)]\t KLD Loss: 463.806976 \t NLL Loss: 25900.548828\n",
      "Train Epoch: 77 [1760/2052 (5%)]\t KLD Loss: 4.877018 \t NLL Loss: 340.281219\n",
      "Train Epoch: 77 [1920/2052 (6%)]\t KLD Loss: 41.986610 \t NLL Loss: 6265.390137\n",
      "Train Epoch: 78 [160/2052 (0%)]\t KLD Loss: 12.620939 \t NLL Loss: 2126.596436\n",
      "Train Epoch: 78 [320/2052 (1%)]\t KLD Loss: 30.244661 \t NLL Loss: 9504.671875\n",
      "Train Epoch: 78 [480/2052 (1%)]\t KLD Loss: 34.282974 \t NLL Loss: 5828.929688\n",
      "Train Epoch: 78 [640/2052 (2%)]\t KLD Loss: 8.375933 \t NLL Loss: 1772.558838\n",
      "Train Epoch: 78 [800/2052 (2%)]\t KLD Loss: 10.254269 \t NLL Loss: 3099.406250\n",
      "Train Epoch: 78 [960/2052 (3%)]\t KLD Loss: 12.629022 \t NLL Loss: 1422.929565\n",
      "Train Epoch: 78 [1120/2052 (3%)]\t KLD Loss: 31.299267 \t NLL Loss: 62386.078125\n",
      "Train Epoch: 78 [1280/2052 (4%)]\t KLD Loss: 27.012520 \t NLL Loss: 3573.830322\n",
      "Train Epoch: 78 [1440/2052 (4%)]\t KLD Loss: 22.108765 \t NLL Loss: 7193.240234\n",
      "Train Epoch: 78 [1600/2052 (5%)]\t KLD Loss: 482.033752 \t NLL Loss: 53881.066406\n",
      "Train Epoch: 78 [1760/2052 (5%)]\t KLD Loss: 4.641107 \t NLL Loss: 331.536377\n",
      "Train Epoch: 78 [1920/2052 (6%)]\t KLD Loss: 43.094681 \t NLL Loss: 6259.410645\n",
      "Train Epoch: 79 [160/2052 (0%)]\t KLD Loss: 12.162493 \t NLL Loss: 2517.785645\n",
      "Train Epoch: 79 [320/2052 (1%)]\t KLD Loss: 36.798981 \t NLL Loss: 9186.851562\n",
      "Train Epoch: 79 [480/2052 (1%)]\t KLD Loss: 40.026573 \t NLL Loss: 5684.975586\n",
      "Train Epoch: 79 [640/2052 (2%)]\t KLD Loss: 9.432034 \t NLL Loss: 1757.817749\n",
      "Train Epoch: 79 [800/2052 (2%)]\t KLD Loss: 10.724428 \t NLL Loss: 2777.042969\n",
      "Train Epoch: 79 [960/2052 (3%)]\t KLD Loss: 12.603275 \t NLL Loss: 3019.889648\n",
      "Train Epoch: 79 [1120/2052 (3%)]\t KLD Loss: 31.427862 \t NLL Loss: 62263.781250\n",
      "Train Epoch: 79 [1280/2052 (4%)]\t KLD Loss: 26.174042 \t NLL Loss: 3642.793945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 [1440/2052 (4%)]\t KLD Loss: 21.674313 \t NLL Loss: 7170.306641\n",
      "Train Epoch: 79 [1600/2052 (5%)]\t KLD Loss: 421.450562 \t NLL Loss: 41054.781250\n",
      "Train Epoch: 79 [1760/2052 (5%)]\t KLD Loss: 4.412478 \t NLL Loss: 346.918884\n",
      "Train Epoch: 79 [1920/2052 (6%)]\t KLD Loss: 39.064072 \t NLL Loss: 6468.431641\n",
      "Train Epoch: 80 [160/2052 (0%)]\t KLD Loss: 13.611105 \t NLL Loss: 2369.324219\n",
      "Train Epoch: 80 [320/2052 (1%)]\t KLD Loss: 34.023605 \t NLL Loss: 9721.599609\n",
      "Train Epoch: 80 [480/2052 (1%)]\t KLD Loss: 41.514793 \t NLL Loss: 5751.691406\n",
      "Train Epoch: 80 [640/2052 (2%)]\t KLD Loss: 9.762292 \t NLL Loss: 1739.406128\n",
      "Train Epoch: 80 [800/2052 (2%)]\t KLD Loss: 10.481175 \t NLL Loss: 2925.125977\n",
      "Train Epoch: 80 [960/2052 (3%)]\t KLD Loss: 13.370513 \t NLL Loss: 2667.244141\n",
      "Train Epoch: 80 [1120/2052 (3%)]\t KLD Loss: 33.872917 \t NLL Loss: 61277.128906\n",
      "Train Epoch: 80 [1280/2052 (4%)]\t KLD Loss: 28.503614 \t NLL Loss: 3848.431152\n",
      "Train Epoch: 80 [1440/2052 (4%)]\t KLD Loss: 22.937647 \t NLL Loss: 7431.551758\n",
      "Train Epoch: 80 [1600/2052 (5%)]\t KLD Loss: 586.819580 \t NLL Loss: 46731.660156\n",
      "Train Epoch: 80 [1760/2052 (5%)]\t KLD Loss: 4.607809 \t NLL Loss: 332.533203\n",
      "Train Epoch: 80 [1920/2052 (6%)]\t KLD Loss: 44.473663 \t NLL Loss: 6011.496094\n",
      "Train Epoch: 81 [160/2052 (0%)]\t KLD Loss: 13.058527 \t NLL Loss: 2259.580566\n",
      "Train Epoch: 81 [320/2052 (1%)]\t KLD Loss: 35.820850 \t NLL Loss: 9581.832031\n",
      "Train Epoch: 81 [480/2052 (1%)]\t KLD Loss: 42.382767 \t NLL Loss: 5670.363281\n",
      "Train Epoch: 81 [640/2052 (2%)]\t KLD Loss: 10.071348 \t NLL Loss: 1762.225586\n",
      "Train Epoch: 81 [800/2052 (2%)]\t KLD Loss: 10.294024 \t NLL Loss: 2840.996094\n",
      "Train Epoch: 81 [960/2052 (3%)]\t KLD Loss: 12.974722 \t NLL Loss: 1423.735352\n",
      "Train Epoch: 81 [1120/2052 (3%)]\t KLD Loss: 33.273975 \t NLL Loss: 61421.726562\n",
      "Train Epoch: 81 [1280/2052 (4%)]\t KLD Loss: 26.665764 \t NLL Loss: 3345.047363\n",
      "Train Epoch: 81 [1440/2052 (4%)]\t KLD Loss: 19.407885 \t NLL Loss: 6657.975098\n",
      "Train Epoch: 81 [1600/2052 (5%)]\t KLD Loss: 373.406342 \t NLL Loss: 42357.101562\n",
      "Train Epoch: 81 [1760/2052 (5%)]\t KLD Loss: 4.747321 \t NLL Loss: 326.698975\n",
      "Train Epoch: 81 [1920/2052 (6%)]\t KLD Loss: 41.876640 \t NLL Loss: 6063.469238\n",
      "Train Epoch: 82 [160/2052 (0%)]\t KLD Loss: 13.471581 \t NLL Loss: 1996.849609\n",
      "Train Epoch: 82 [320/2052 (1%)]\t KLD Loss: 38.086929 \t NLL Loss: 8975.869141\n",
      "Train Epoch: 82 [480/2052 (1%)]\t KLD Loss: 40.477024 \t NLL Loss: 5725.309570\n",
      "Train Epoch: 82 [640/2052 (2%)]\t KLD Loss: 9.046756 \t NLL Loss: 1734.505615\n",
      "Train Epoch: 82 [800/2052 (2%)]\t KLD Loss: 9.840219 \t NLL Loss: 2737.481934\n",
      "Train Epoch: 82 [960/2052 (3%)]\t KLD Loss: 11.942230 \t NLL Loss: 1700.205444\n",
      "Train Epoch: 82 [1120/2052 (3%)]\t KLD Loss: 29.914888 \t NLL Loss: 61328.835938\n",
      "Train Epoch: 82 [1280/2052 (4%)]\t KLD Loss: 27.515789 \t NLL Loss: 3623.536621\n",
      "Train Epoch: 82 [1440/2052 (4%)]\t KLD Loss: 22.607460 \t NLL Loss: 8132.104004\n",
      "Train Epoch: 82 [1600/2052 (5%)]\t KLD Loss: 436.439453 \t NLL Loss: 28226.378906\n",
      "Train Epoch: 82 [1760/2052 (5%)]\t KLD Loss: 4.630172 \t NLL Loss: 324.201019\n",
      "Train Epoch: 82 [1920/2052 (6%)]\t KLD Loss: 42.165710 \t NLL Loss: 6033.337402\n",
      "Train Epoch: 83 [160/2052 (0%)]\t KLD Loss: 11.856221 \t NLL Loss: 2066.333496\n",
      "Train Epoch: 83 [320/2052 (1%)]\t KLD Loss: 33.635296 \t NLL Loss: 9223.144531\n",
      "Train Epoch: 83 [480/2052 (1%)]\t KLD Loss: 37.567322 \t NLL Loss: 5621.524414\n",
      "Train Epoch: 83 [640/2052 (2%)]\t KLD Loss: 9.153484 \t NLL Loss: 1746.368408\n",
      "Train Epoch: 83 [800/2052 (2%)]\t KLD Loss: 10.463254 \t NLL Loss: 2879.317383\n",
      "Train Epoch: 83 [960/2052 (3%)]\t KLD Loss: 13.534648 \t NLL Loss: 1828.564087\n",
      "Train Epoch: 83 [1120/2052 (3%)]\t KLD Loss: 33.080517 \t NLL Loss: 61483.144531\n",
      "Train Epoch: 83 [1280/2052 (4%)]\t KLD Loss: 28.092461 \t NLL Loss: 3683.582031\n",
      "Train Epoch: 83 [1440/2052 (4%)]\t KLD Loss: 22.798161 \t NLL Loss: 6382.495605\n",
      "Train Epoch: 83 [1600/2052 (5%)]\t KLD Loss: 518.234802 \t NLL Loss: 51913.476562\n",
      "Train Epoch: 83 [1760/2052 (5%)]\t KLD Loss: 4.727694 \t NLL Loss: 366.701660\n",
      "Train Epoch: 83 [1920/2052 (6%)]\t KLD Loss: 39.007347 \t NLL Loss: 6131.413086\n",
      "Train Epoch: 84 [160/2052 (0%)]\t KLD Loss: 13.298759 \t NLL Loss: 2142.887207\n",
      "Train Epoch: 84 [320/2052 (1%)]\t KLD Loss: 38.672615 \t NLL Loss: 8798.559570\n",
      "Train Epoch: 84 [480/2052 (1%)]\t KLD Loss: 41.819149 \t NLL Loss: 5475.582031\n",
      "Train Epoch: 84 [640/2052 (2%)]\t KLD Loss: 8.737092 \t NLL Loss: 1750.545532\n",
      "Train Epoch: 84 [800/2052 (2%)]\t KLD Loss: 9.839258 \t NLL Loss: 2839.523438\n",
      "Train Epoch: 84 [960/2052 (3%)]\t KLD Loss: 12.770927 \t NLL Loss: 2000.833252\n",
      "Train Epoch: 84 [1120/2052 (3%)]\t KLD Loss: 29.198843 \t NLL Loss: 61288.429688\n",
      "Train Epoch: 84 [1280/2052 (4%)]\t KLD Loss: 24.779127 \t NLL Loss: 3400.043213\n",
      "Train Epoch: 84 [1440/2052 (4%)]\t KLD Loss: 24.268795 \t NLL Loss: 6527.664062\n",
      "Train Epoch: 84 [1600/2052 (5%)]\t KLD Loss: 387.802795 \t NLL Loss: 32711.980469\n",
      "Train Epoch: 84 [1760/2052 (5%)]\t KLD Loss: 4.967690 \t NLL Loss: 320.268555\n",
      "Train Epoch: 84 [1920/2052 (6%)]\t KLD Loss: 42.727379 \t NLL Loss: 5778.496582\n",
      "Train Epoch: 85 [160/2052 (0%)]\t KLD Loss: 13.475624 \t NLL Loss: 2318.100586\n",
      "Train Epoch: 85 [320/2052 (1%)]\t KLD Loss: 37.203751 \t NLL Loss: 8529.384766\n",
      "Train Epoch: 85 [480/2052 (1%)]\t KLD Loss: 42.019142 \t NLL Loss: 5497.114258\n",
      "Train Epoch: 85 [640/2052 (2%)]\t KLD Loss: 9.507062 \t NLL Loss: 1752.453003\n",
      "Train Epoch: 85 [800/2052 (2%)]\t KLD Loss: 10.284911 \t NLL Loss: 2732.041504\n",
      "Train Epoch: 85 [960/2052 (3%)]\t KLD Loss: 11.824074 \t NLL Loss: 2205.502930\n",
      "Train Epoch: 85 [1120/2052 (3%)]\t KLD Loss: 33.386322 \t NLL Loss: 61551.199219\n",
      "Train Epoch: 85 [1280/2052 (4%)]\t KLD Loss: 28.801044 \t NLL Loss: 3453.487793\n",
      "Train Epoch: 85 [1440/2052 (4%)]\t KLD Loss: 25.086737 \t NLL Loss: 11276.539062\n",
      "Train Epoch: 85 [1600/2052 (5%)]\t KLD Loss: 436.543915 \t NLL Loss: 30200.351562\n",
      "Train Epoch: 85 [1760/2052 (5%)]\t KLD Loss: 4.687369 \t NLL Loss: 335.596100\n",
      "Train Epoch: 85 [1920/2052 (6%)]\t KLD Loss: 42.383667 \t NLL Loss: 5882.494629\n",
      "Train Epoch: 86 [160/2052 (0%)]\t KLD Loss: 12.775944 \t NLL Loss: 1997.930786\n",
      "Train Epoch: 86 [320/2052 (1%)]\t KLD Loss: 35.783798 \t NLL Loss: 9316.847656\n",
      "Train Epoch: 86 [480/2052 (1%)]\t KLD Loss: 39.230446 \t NLL Loss: 5513.763184\n",
      "Train Epoch: 86 [640/2052 (2%)]\t KLD Loss: 9.027405 \t NLL Loss: 1709.994263\n",
      "Train Epoch: 86 [800/2052 (2%)]\t KLD Loss: 9.645388 \t NLL Loss: 2880.616699\n",
      "Train Epoch: 86 [960/2052 (3%)]\t KLD Loss: 9.721035 \t NLL Loss: 2419.243652\n",
      "Train Epoch: 86 [1120/2052 (3%)]\t KLD Loss: 34.176937 \t NLL Loss: 61405.343750\n",
      "Train Epoch: 86 [1280/2052 (4%)]\t KLD Loss: 26.792843 \t NLL Loss: 3443.699951\n",
      "Train Epoch: 86 [1440/2052 (4%)]\t KLD Loss: 22.220444 \t NLL Loss: 6915.717285\n",
      "Train Epoch: 86 [1600/2052 (5%)]\t KLD Loss: 331.633606 \t NLL Loss: 90298.789062\n",
      "Train Epoch: 86 [1760/2052 (5%)]\t KLD Loss: 4.058130 \t NLL Loss: 336.082916\n",
      "Train Epoch: 86 [1920/2052 (6%)]\t KLD Loss: 41.115814 \t NLL Loss: 5790.003906\n",
      "Train Epoch: 87 [160/2052 (0%)]\t KLD Loss: 12.888860 \t NLL Loss: 2323.865479\n",
      "Train Epoch: 87 [320/2052 (1%)]\t KLD Loss: 35.067688 \t NLL Loss: 8714.806641\n",
      "Train Epoch: 87 [480/2052 (1%)]\t KLD Loss: 38.524315 \t NLL Loss: 5515.380859\n",
      "Train Epoch: 87 [640/2052 (2%)]\t KLD Loss: 9.209802 \t NLL Loss: 1730.889404\n",
      "Train Epoch: 87 [800/2052 (2%)]\t KLD Loss: 10.543030 \t NLL Loss: 2925.608887\n",
      "Train Epoch: 87 [960/2052 (3%)]\t KLD Loss: 11.141951 \t NLL Loss: 1405.291992\n",
      "Train Epoch: 87 [1120/2052 (3%)]\t KLD Loss: 36.095554 \t NLL Loss: 61039.613281\n",
      "Train Epoch: 87 [1280/2052 (4%)]\t KLD Loss: 29.648705 \t NLL Loss: 3415.056152\n",
      "Train Epoch: 87 [1440/2052 (4%)]\t KLD Loss: 25.940926 \t NLL Loss: 6280.719238\n",
      "Train Epoch: 87 [1600/2052 (5%)]\t KLD Loss: 367.616852 \t NLL Loss: 33528.023438\n",
      "Train Epoch: 87 [1760/2052 (5%)]\t KLD Loss: 4.534116 \t NLL Loss: 324.096130\n",
      "Train Epoch: 87 [1920/2052 (6%)]\t KLD Loss: 38.268154 \t NLL Loss: 5914.055664\n",
      "Train Epoch: 88 [160/2052 (0%)]\t KLD Loss: 12.916525 \t NLL Loss: 1992.288208\n",
      "Train Epoch: 88 [320/2052 (1%)]\t KLD Loss: 33.429237 \t NLL Loss: 8555.009766\n",
      "Train Epoch: 88 [480/2052 (1%)]\t KLD Loss: 38.114441 \t NLL Loss: 5489.630859\n",
      "Train Epoch: 88 [640/2052 (2%)]\t KLD Loss: 8.507071 \t NLL Loss: 1721.823242\n",
      "Train Epoch: 88 [800/2052 (2%)]\t KLD Loss: 8.840822 \t NLL Loss: 2821.589355\n",
      "Train Epoch: 88 [960/2052 (3%)]\t KLD Loss: 10.888213 \t NLL Loss: 1182.070679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 88 [1120/2052 (3%)]\t KLD Loss: 30.857353 \t NLL Loss: 60772.890625\n",
      "Train Epoch: 88 [1280/2052 (4%)]\t KLD Loss: 23.027000 \t NLL Loss: 3464.516602\n",
      "Train Epoch: 88 [1440/2052 (4%)]\t KLD Loss: 23.560493 \t NLL Loss: 5717.440430\n",
      "Train Epoch: 88 [1600/2052 (5%)]\t KLD Loss: 471.769379 \t NLL Loss: 35398.082031\n",
      "Train Epoch: 88 [1760/2052 (5%)]\t KLD Loss: 4.418157 \t NLL Loss: 305.786713\n",
      "Train Epoch: 88 [1920/2052 (6%)]\t KLD Loss: 40.826347 \t NLL Loss: 5742.542480\n",
      "Train Epoch: 89 [160/2052 (0%)]\t KLD Loss: 13.251925 \t NLL Loss: 2011.820312\n",
      "Train Epoch: 89 [320/2052 (1%)]\t KLD Loss: 34.732834 \t NLL Loss: 8472.835938\n",
      "Train Epoch: 89 [480/2052 (1%)]\t KLD Loss: 40.288277 \t NLL Loss: 5449.512695\n",
      "Train Epoch: 89 [640/2052 (2%)]\t KLD Loss: 8.795683 \t NLL Loss: 1688.958252\n",
      "Train Epoch: 89 [800/2052 (2%)]\t KLD Loss: 8.861351 \t NLL Loss: 2815.557617\n",
      "Train Epoch: 89 [960/2052 (3%)]\t KLD Loss: 10.195263 \t NLL Loss: 1111.078857\n",
      "Train Epoch: 89 [1120/2052 (3%)]\t KLD Loss: 33.322277 \t NLL Loss: 60460.292969\n",
      "Train Epoch: 89 [1280/2052 (4%)]\t KLD Loss: 24.357424 \t NLL Loss: 3646.872559\n",
      "Train Epoch: 89 [1440/2052 (4%)]\t KLD Loss: 23.524292 \t NLL Loss: 8334.324219\n",
      "Train Epoch: 89 [1600/2052 (5%)]\t KLD Loss: 327.841858 \t NLL Loss: 33723.933594\n",
      "Train Epoch: 89 [1760/2052 (5%)]\t KLD Loss: 4.354160 \t NLL Loss: 292.524109\n",
      "Train Epoch: 89 [1920/2052 (6%)]\t KLD Loss: 37.493378 \t NLL Loss: 5752.548828\n",
      "Train Epoch: 90 [160/2052 (0%)]\t KLD Loss: 10.217129 \t NLL Loss: 2869.486572\n",
      "Train Epoch: 90 [320/2052 (1%)]\t KLD Loss: 33.814930 \t NLL Loss: 8287.078125\n",
      "Train Epoch: 90 [480/2052 (1%)]\t KLD Loss: 35.350056 \t NLL Loss: 5529.147949\n",
      "Train Epoch: 90 [640/2052 (2%)]\t KLD Loss: 8.662451 \t NLL Loss: 1677.655518\n",
      "Train Epoch: 90 [800/2052 (2%)]\t KLD Loss: 9.119610 \t NLL Loss: 2834.822266\n",
      "Train Epoch: 90 [960/2052 (3%)]\t KLD Loss: 8.587760 \t NLL Loss: 1241.291138\n",
      "Train Epoch: 90 [1120/2052 (3%)]\t KLD Loss: 31.446440 \t NLL Loss: 60876.062500\n",
      "Train Epoch: 90 [1280/2052 (4%)]\t KLD Loss: 26.628387 \t NLL Loss: 3236.634766\n",
      "Train Epoch: 90 [1440/2052 (4%)]\t KLD Loss: 24.998892 \t NLL Loss: 6026.765625\n",
      "Train Epoch: 90 [1600/2052 (5%)]\t KLD Loss: 450.498352 \t NLL Loss: 39609.539062\n",
      "Train Epoch: 90 [1760/2052 (5%)]\t KLD Loss: 4.739094 \t NLL Loss: 331.161896\n",
      "Train Epoch: 90 [1920/2052 (6%)]\t KLD Loss: 43.544815 \t NLL Loss: 5535.350098\n",
      "Train Epoch: 91 [160/2052 (0%)]\t KLD Loss: 12.794193 \t NLL Loss: 2304.824219\n",
      "Train Epoch: 91 [320/2052 (1%)]\t KLD Loss: 34.463074 \t NLL Loss: 8524.917969\n",
      "Train Epoch: 91 [480/2052 (1%)]\t KLD Loss: 40.164536 \t NLL Loss: 5376.181641\n",
      "Train Epoch: 91 [640/2052 (2%)]\t KLD Loss: 9.041666 \t NLL Loss: 1728.551392\n",
      "Train Epoch: 91 [800/2052 (2%)]\t KLD Loss: 9.069104 \t NLL Loss: 2839.380371\n",
      "Train Epoch: 91 [960/2052 (3%)]\t KLD Loss: 10.253451 \t NLL Loss: 2082.137695\n",
      "Train Epoch: 91 [1120/2052 (3%)]\t KLD Loss: 30.863470 \t NLL Loss: 60471.964844\n",
      "Train Epoch: 91 [1280/2052 (4%)]\t KLD Loss: 23.227776 \t NLL Loss: 3652.281738\n",
      "Train Epoch: 91 [1440/2052 (4%)]\t KLD Loss: 26.511742 \t NLL Loss: 5274.515625\n",
      "Train Epoch: 91 [1600/2052 (5%)]\t KLD Loss: 371.165924 \t NLL Loss: 37313.089844\n",
      "Train Epoch: 91 [1760/2052 (5%)]\t KLD Loss: 4.376834 \t NLL Loss: 319.953278\n",
      "Train Epoch: 91 [1920/2052 (6%)]\t KLD Loss: 40.190731 \t NLL Loss: 5642.886719\n",
      "Train Epoch: 92 [160/2052 (0%)]\t KLD Loss: 12.168910 \t NLL Loss: 2094.382568\n",
      "Train Epoch: 92 [320/2052 (1%)]\t KLD Loss: 32.844688 \t NLL Loss: 8045.895508\n",
      "Train Epoch: 92 [480/2052 (1%)]\t KLD Loss: 37.303604 \t NLL Loss: 5441.117188\n",
      "Train Epoch: 92 [640/2052 (2%)]\t KLD Loss: 8.853046 \t NLL Loss: 1711.516846\n",
      "Train Epoch: 92 [800/2052 (2%)]\t KLD Loss: 9.926800 \t NLL Loss: 2737.981689\n",
      "Train Epoch: 92 [960/2052 (3%)]\t KLD Loss: 8.834011 \t NLL Loss: 2148.821289\n",
      "Train Epoch: 92 [1120/2052 (3%)]\t KLD Loss: 32.893147 \t NLL Loss: 60536.000000\n",
      "Train Epoch: 92 [1280/2052 (4%)]\t KLD Loss: 30.315784 \t NLL Loss: 3763.107422\n",
      "Train Epoch: 92 [1440/2052 (4%)]\t KLD Loss: 24.067570 \t NLL Loss: 10363.300781\n",
      "Train Epoch: 92 [1600/2052 (5%)]\t KLD Loss: 555.361389 \t NLL Loss: 61658.593750\n",
      "Train Epoch: 92 [1760/2052 (5%)]\t KLD Loss: 4.541564 \t NLL Loss: 281.370972\n",
      "Train Epoch: 92 [1920/2052 (6%)]\t KLD Loss: 39.173687 \t NLL Loss: 5524.744141\n",
      "Train Epoch: 93 [160/2052 (0%)]\t KLD Loss: 12.711279 \t NLL Loss: 1798.219849\n",
      "Train Epoch: 93 [320/2052 (1%)]\t KLD Loss: 34.528011 \t NLL Loss: 8558.724609\n",
      "Train Epoch: 93 [480/2052 (1%)]\t KLD Loss: 36.197327 \t NLL Loss: 5401.329102\n",
      "Train Epoch: 93 [640/2052 (2%)]\t KLD Loss: 8.437500 \t NLL Loss: 1723.923340\n",
      "Train Epoch: 93 [800/2052 (2%)]\t KLD Loss: 10.353276 \t NLL Loss: 2669.670898\n",
      "Train Epoch: 93 [960/2052 (3%)]\t KLD Loss: 10.274132 \t NLL Loss: 1607.581299\n",
      "Train Epoch: 93 [1120/2052 (3%)]\t KLD Loss: 32.890038 \t NLL Loss: 59850.367188\n",
      "Train Epoch: 93 [1280/2052 (4%)]\t KLD Loss: 28.361689 \t NLL Loss: 3170.532471\n",
      "Train Epoch: 93 [1440/2052 (4%)]\t KLD Loss: 25.284786 \t NLL Loss: 7139.687500\n",
      "Train Epoch: 93 [1600/2052 (5%)]\t KLD Loss: 302.289185 \t NLL Loss: 58924.386719\n",
      "Train Epoch: 93 [1760/2052 (5%)]\t KLD Loss: 4.817677 \t NLL Loss: 273.736053\n",
      "Train Epoch: 93 [1920/2052 (6%)]\t KLD Loss: 38.359097 \t NLL Loss: 5478.166016\n",
      "Train Epoch: 94 [160/2052 (0%)]\t KLD Loss: 13.204683 \t NLL Loss: 1984.024170\n",
      "Train Epoch: 94 [320/2052 (1%)]\t KLD Loss: 36.284027 \t NLL Loss: 8684.359375\n",
      "Train Epoch: 94 [480/2052 (1%)]\t KLD Loss: 37.377857 \t NLL Loss: 5433.563477\n",
      "Train Epoch: 94 [640/2052 (2%)]\t KLD Loss: 8.092442 \t NLL Loss: 1664.900879\n",
      "Train Epoch: 94 [800/2052 (2%)]\t KLD Loss: 8.845108 \t NLL Loss: 2711.054199\n",
      "Train Epoch: 94 [960/2052 (3%)]\t KLD Loss: 10.480075 \t NLL Loss: 1110.103027\n",
      "Train Epoch: 94 [1120/2052 (3%)]\t KLD Loss: 32.943985 \t NLL Loss: 60214.363281\n",
      "Train Epoch: 94 [1280/2052 (4%)]\t KLD Loss: 26.671320 \t NLL Loss: 3242.465820\n",
      "Train Epoch: 94 [1440/2052 (4%)]\t KLD Loss: 26.283375 \t NLL Loss: 5697.052246\n",
      "Train Epoch: 94 [1600/2052 (5%)]\t KLD Loss: 461.664001 \t NLL Loss: 34933.820312\n",
      "Train Epoch: 94 [1760/2052 (5%)]\t KLD Loss: 5.031107 \t NLL Loss: 286.936584\n",
      "Train Epoch: 94 [1920/2052 (6%)]\t KLD Loss: 42.071117 \t NLL Loss: 5382.003906\n",
      "Train Epoch: 95 [160/2052 (0%)]\t KLD Loss: 12.377983 \t NLL Loss: 2713.353027\n",
      "Train Epoch: 95 [320/2052 (1%)]\t KLD Loss: 36.182236 \t NLL Loss: 7987.493164\n",
      "Train Epoch: 95 [480/2052 (1%)]\t KLD Loss: 40.899071 \t NLL Loss: 5290.653809\n",
      "Train Epoch: 95 [640/2052 (2%)]\t KLD Loss: 9.145891 \t NLL Loss: 1727.254150\n",
      "Train Epoch: 95 [800/2052 (2%)]\t KLD Loss: 9.641853 \t NLL Loss: 3017.905762\n",
      "Train Epoch: 95 [960/2052 (3%)]\t KLD Loss: 13.284894 \t NLL Loss: 2488.879883\n",
      "Train Epoch: 95 [1120/2052 (3%)]\t KLD Loss: 34.077358 \t NLL Loss: 61364.085938\n",
      "Train Epoch: 95 [1280/2052 (4%)]\t KLD Loss: 24.922220 \t NLL Loss: 3283.639160\n",
      "Train Epoch: 95 [1440/2052 (4%)]\t KLD Loss: 26.371891 \t NLL Loss: 5414.973633\n",
      "Train Epoch: 95 [1600/2052 (5%)]\t KLD Loss: 308.369385 \t NLL Loss: 59640.320312\n",
      "Train Epoch: 95 [1760/2052 (5%)]\t KLD Loss: 4.307100 \t NLL Loss: 296.644012\n",
      "Train Epoch: 95 [1920/2052 (6%)]\t KLD Loss: 39.455177 \t NLL Loss: 5539.185547\n",
      "Train Epoch: 96 [160/2052 (0%)]\t KLD Loss: 13.632914 \t NLL Loss: 2022.752197\n",
      "Train Epoch: 96 [320/2052 (1%)]\t KLD Loss: 35.056667 \t NLL Loss: 7935.236328\n",
      "Train Epoch: 96 [480/2052 (1%)]\t KLD Loss: 38.245350 \t NLL Loss: 5316.472656\n",
      "Train Epoch: 96 [640/2052 (2%)]\t KLD Loss: 8.964382 \t NLL Loss: 1639.032715\n",
      "Train Epoch: 96 [800/2052 (2%)]\t KLD Loss: 9.227615 \t NLL Loss: 2736.900879\n",
      "Train Epoch: 96 [960/2052 (3%)]\t KLD Loss: 13.104773 \t NLL Loss: 2129.800293\n",
      "Train Epoch: 96 [1120/2052 (3%)]\t KLD Loss: 32.332420 \t NLL Loss: 60257.843750\n",
      "Train Epoch: 96 [1280/2052 (4%)]\t KLD Loss: 27.971626 \t NLL Loss: 3454.480957\n",
      "Train Epoch: 96 [1440/2052 (4%)]\t KLD Loss: 27.053709 \t NLL Loss: 4905.821777\n",
      "Train Epoch: 96 [1600/2052 (5%)]\t KLD Loss: 584.015503 \t NLL Loss: 49429.203125\n",
      "Train Epoch: 96 [1760/2052 (5%)]\t KLD Loss: 5.165476 \t NLL Loss: 289.165344\n",
      "Train Epoch: 96 [1920/2052 (6%)]\t KLD Loss: 43.134762 \t NLL Loss: 5393.046875\n",
      "Train Epoch: 97 [160/2052 (0%)]\t KLD Loss: 12.949626 \t NLL Loss: 1810.895508\n",
      "Train Epoch: 97 [320/2052 (1%)]\t KLD Loss: 35.962013 \t NLL Loss: 7568.232422\n",
      "Train Epoch: 97 [480/2052 (1%)]\t KLD Loss: 39.498726 \t NLL Loss: 5329.955566\n",
      "Train Epoch: 97 [640/2052 (2%)]\t KLD Loss: 9.246246 \t NLL Loss: 1735.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 97 [800/2052 (2%)]\t KLD Loss: 9.148701 \t NLL Loss: 2727.740723\n",
      "Train Epoch: 97 [960/2052 (3%)]\t KLD Loss: 11.545479 \t NLL Loss: 1169.976074\n",
      "Train Epoch: 97 [1120/2052 (3%)]\t KLD Loss: 31.828962 \t NLL Loss: 59712.417969\n",
      "Train Epoch: 97 [1280/2052 (4%)]\t KLD Loss: 26.843922 \t NLL Loss: 3237.654053\n",
      "Train Epoch: 97 [1440/2052 (4%)]\t KLD Loss: 25.714470 \t NLL Loss: 4895.186523\n",
      "Train Epoch: 97 [1600/2052 (5%)]\t KLD Loss: 283.932129 \t NLL Loss: 83139.554688\n",
      "Train Epoch: 97 [1760/2052 (5%)]\t KLD Loss: 4.429502 \t NLL Loss: 243.756561\n",
      "Train Epoch: 97 [1920/2052 (6%)]\t KLD Loss: 45.254189 \t NLL Loss: 5470.403320\n",
      "Train Epoch: 98 [160/2052 (0%)]\t KLD Loss: 13.285866 \t NLL Loss: 2011.718506\n",
      "Train Epoch: 98 [320/2052 (1%)]\t KLD Loss: 39.722221 \t NLL Loss: 8625.367188\n",
      "Train Epoch: 98 [480/2052 (1%)]\t KLD Loss: 40.160770 \t NLL Loss: 6303.870117\n",
      "Train Epoch: 98 [640/2052 (2%)]\t KLD Loss: 9.098618 \t NLL Loss: 1664.441406\n",
      "Train Epoch: 98 [800/2052 (2%)]\t KLD Loss: 10.345669 \t NLL Loss: 2765.933105\n",
      "Train Epoch: 98 [960/2052 (3%)]\t KLD Loss: 11.014851 \t NLL Loss: 1004.156555\n",
      "Train Epoch: 98 [1120/2052 (3%)]\t KLD Loss: 32.529934 \t NLL Loss: 58994.394531\n",
      "Train Epoch: 98 [1280/2052 (4%)]\t KLD Loss: 25.718525 \t NLL Loss: 3883.535400\n",
      "Train Epoch: 98 [1440/2052 (4%)]\t KLD Loss: 27.474567 \t NLL Loss: 6285.963867\n",
      "Train Epoch: 98 [1600/2052 (5%)]\t KLD Loss: 445.811279 \t NLL Loss: 34772.390625\n",
      "Train Epoch: 98 [1760/2052 (5%)]\t KLD Loss: 4.387691 \t NLL Loss: 283.975555\n",
      "Train Epoch: 98 [1920/2052 (6%)]\t KLD Loss: 39.108608 \t NLL Loss: 5505.215820\n",
      "Train Epoch: 99 [160/2052 (0%)]\t KLD Loss: 13.668455 \t NLL Loss: 2576.430664\n",
      "Train Epoch: 99 [320/2052 (1%)]\t KLD Loss: 36.951595 \t NLL Loss: 8501.166016\n",
      "Train Epoch: 99 [480/2052 (1%)]\t KLD Loss: 39.032932 \t NLL Loss: 5377.144531\n",
      "Train Epoch: 99 [640/2052 (2%)]\t KLD Loss: 9.235579 \t NLL Loss: 1720.581543\n",
      "Train Epoch: 99 [800/2052 (2%)]\t KLD Loss: 10.559190 \t NLL Loss: 2849.496338\n",
      "Train Epoch: 99 [960/2052 (3%)]\t KLD Loss: 10.728936 \t NLL Loss: 1294.799316\n",
      "Train Epoch: 99 [1120/2052 (3%)]\t KLD Loss: 32.973602 \t NLL Loss: 59604.566406\n",
      "Train Epoch: 99 [1280/2052 (4%)]\t KLD Loss: 28.874804 \t NLL Loss: 3152.471924\n",
      "Train Epoch: 99 [1440/2052 (4%)]\t KLD Loss: 27.872782 \t NLL Loss: 5114.193359\n",
      "Train Epoch: 99 [1600/2052 (5%)]\t KLD Loss: 354.040771 \t NLL Loss: 72966.312500\n",
      "Train Epoch: 99 [1760/2052 (5%)]\t KLD Loss: 4.746468 \t NLL Loss: 285.995728\n",
      "Train Epoch: 99 [1920/2052 (6%)]\t KLD Loss: 42.039864 \t NLL Loss: 5386.533203\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    batch_idx = 0   \n",
    "    for batch_data, pad_data in loader.next_batch(batch_size):\n",
    "        batch_idx += 1\n",
    "\n",
    "        batch_data = batch_data.to(device)\n",
    "        pad_data = pad_data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        kld_loss, nll_loss, _, _ = model(batch_data, pad_data)\n",
    "        loss = kld_loss + nll_loss\n",
    "        loss.backward()\n",
    "\n",
    "        #grad norm clipping, only in pytorch version >= 1.10\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        #printing\n",
    "        if batch_idx % print_every == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t KLD Loss: {:.6f} \\t NLL Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(batch_data), len(loader.train_data),\n",
    "                100. * batch_idx / len(loader.train_data),\n",
    "                kld_loss.data / batch_size,\n",
    "                nll_loss.data / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3399e+00,  1.5883e+00,  8.7096e-01,  2.9367e+00, -7.4408e+00,\n",
       "         -7.1307e-01,  9.7113e+00,  4.8143e-01,  1.6757e+00,  3.9058e+00,\n",
       "         -4.7724e-01,  5.3615e-01,  4.5017e+00, -8.6025e-01,  1.0346e+00,\n",
       "         -3.2432e+00, -1.5993e-01, -2.3737e-01,  7.3380e+00, -1.1106e+00,\n",
       "         -1.9674e-02,  5.4773e+01,  7.3220e+00,  3.1609e+01, -2.3862e-01,\n",
       "         -7.2630e+00,  1.3165e+02, -2.1035e+01,  1.6367e+01, -2.0119e+01,\n",
       "          9.2885e+00,  1.1259e+00, -1.2379e+00,  6.0602e+01, -4.2675e+00,\n",
       "         -2.5331e+01,  1.7882e+00,  1.4614e+00, -1.2955e+02, -2.5077e+01,\n",
       "         -1.0907e+01,  2.2031e+01,  3.1757e+01, -1.5698e+01,  4.8255e+00,\n",
       "         -3.3312e+01, -3.9984e+00,  4.2571e+00,  6.8291e-01, -9.2192e+00,\n",
       "          2.5227e+00,  4.4588e+00, -4.9139e-01,  2.1635e-01, -3.2379e+01,\n",
       "          3.9923e+00, -6.0985e+00, -1.0502e+00,  8.0443e+00,  5.6476e-01,\n",
       "          3.1058e+00, -1.5721e-01,  1.5601e-02, -6.8247e-02, -3.1076e-01,\n",
       "         -1.2861e-01],\n",
       "        [ 1.1853e+00,  1.2895e+00,  1.3573e+01, -1.9436e+01,  1.3155e+01,\n",
       "          3.4799e+00,  1.2243e+01, -3.0559e+00,  2.3344e+00, -1.1006e-01,\n",
       "          5.7941e-01,  1.0014e-01, -3.4985e+00, -8.5767e-01, -5.2053e-01,\n",
       "         -7.1515e+00,  2.9224e+00,  1.2088e+00, -6.9989e-01, -2.1881e+00,\n",
       "         -2.6027e+00,  7.6200e+01,  4.7666e-01, -4.7705e+00,  6.2503e+00,\n",
       "          6.3267e-01,  1.2247e+00, -1.2207e+01, -1.3141e+00, -1.1667e+01,\n",
       "          1.5709e+00, -1.6957e+00, -2.7730e+00,  7.3713e+01, -6.8252e+00,\n",
       "          8.7930e+00,  1.0418e+01, -5.0028e+00,  8.6862e+00, -2.6717e+01,\n",
       "          8.7380e+00,  4.9942e+00,  2.2925e+01, -1.4536e+01,  1.4517e+00,\n",
       "         -4.2811e+01, -1.8623e+00,  6.4788e+00, -1.6932e+01,  2.4946e+00,\n",
       "          1.9622e+00,  8.9451e+00,  6.4135e-01, -2.8348e+00, -3.7069e+01,\n",
       "          4.8414e+00, -1.0886e+00, -7.9976e+00, -5.5989e-01, -2.8293e+00,\n",
       "          2.3090e+00, -1.0008e+00,  1.0704e+00, -6.1852e-01,  5.2158e-01,\n",
       "         -6.2848e-01],\n",
       "        [-1.1673e+00, -4.3740e+00,  3.9571e+00, -5.8886e-01, -4.5767e+00,\n",
       "          1.9611e+00,  8.2302e+00, -3.8350e-01, -8.3980e-01,  6.1782e+00,\n",
       "         -9.9213e-01, -1.7137e-01,  6.7156e+00, -1.0937e+00, -1.1858e+00,\n",
       "         -1.6659e+00,  4.6657e-01, -5.5270e-01,  8.4164e+00,  5.3819e+00,\n",
       "          6.3297e+00,  4.8031e+01, -6.8461e-01,  7.2696e+00,  6.2546e+00,\n",
       "          3.9427e-01,  4.4002e+01,  3.0842e-01,  1.9868e+01, -4.6553e+00,\n",
       "          6.3342e+00, -3.7304e+00, -5.3223e+00,  5.2026e+01,  5.1254e+00,\n",
       "         -2.2773e+00,  5.0988e+00, -1.1569e+00, -4.2831e+01,  4.1557e+00,\n",
       "         -2.0843e+01, -1.5458e+00,  5.9246e+01, -8.9516e+00,  4.6449e+00,\n",
       "         -7.5744e+01,  4.1601e-01,  1.5638e+00, -5.8021e+00, -1.8438e-01,\n",
       "         -3.4996e+00,  4.5960e+00,  1.0065e+00,  2.5692e-01, -7.7265e+01,\n",
       "         -6.4484e-01, -1.8657e+00,  6.6748e-01,  5.4047e+00,  2.2434e+00,\n",
       "          1.5039e+01, -4.5202e-02,  6.6981e-01,  4.2501e-01,  4.5007e-01,\n",
       "          2.8015e-01],\n",
       "        [-1.1155e+00, -7.8522e-01,  4.9586e+00,  6.0071e+00, -1.0107e+01,\n",
       "         -2.4176e+00,  1.0583e+01, -1.9619e+00, -1.1758e+00,  9.4424e+00,\n",
       "         -2.6775e+00,  7.4547e-02,  8.6997e+00, -2.9508e+00,  1.7372e-01,\n",
       "         -2.5075e+00, -1.0747e+00,  5.2491e-04,  5.8126e+00,  1.4863e+00,\n",
       "          2.9197e+00,  6.0604e+01,  2.3506e+00,  1.3473e+01,  4.0222e+00,\n",
       "         -4.3362e+00,  1.1179e+02, -9.2110e+00,  4.1060e+01, -1.3643e+01,\n",
       "          4.9998e+00,  9.0372e-01, -4.3747e+00,  6.7800e+01,  6.2262e+00,\n",
       "         -1.5466e+01,  1.6371e+00,  4.7911e-01, -1.1351e+02, -9.0421e+00,\n",
       "         -3.6174e+01,  5.3008e+00,  5.3700e+01, -1.5106e+01,  6.3647e+00,\n",
       "         -7.0599e+01, -1.1812e+00,  9.5000e-01,  1.0220e+00, -6.6362e+00,\n",
       "         -7.8160e-01,  2.2617e+00,  6.6475e-01,  2.0544e+00, -7.1595e+01,\n",
       "         -1.5345e+00, -2.3877e+00,  4.2624e+00,  7.1434e+00,  3.8965e+00,\n",
       "          1.1547e+01,  3.8264e-01, -5.3714e-02, -2.9885e-01, -5.5716e-02,\n",
       "          2.0755e-01],\n",
       "        [-2.8721e+00, -3.0182e+00,  5.2221e+00, -7.4332e+00, -3.2087e+00,\n",
       "         -3.0119e+00,  4.9597e+00, -7.6838e-01, -2.9192e-01,  3.5556e+00,\n",
       "         -1.0526e+00,  2.7947e-01,  3.4836e+00, -1.1777e+00, -4.6289e-01,\n",
       "         -1.4499e+00,  1.0750e+00, -3.0032e-01,  7.6331e+00,  1.3128e+00,\n",
       "          2.5258e+00,  3.9115e+01, -4.4780e+00,  4.8437e+00,  3.5981e+00,\n",
       "          2.0336e-01,  2.1480e+01,  3.1182e+00,  2.2286e+01, -5.0361e+00,\n",
       "          3.6968e+00, -3.3822e-01, -2.2827e+00,  3.5528e+01,  1.4674e+01,\n",
       "         -2.2232e+00,  1.2301e+00, -8.9135e-02, -2.2696e+01,  5.9725e+00,\n",
       "         -1.9055e+01,  3.6632e+00,  2.8023e+01, -7.7521e+00,  2.2674e+00,\n",
       "         -4.5069e+01, -3.7884e-01, -2.0108e-01, -6.0164e+00,  1.7270e+00,\n",
       "          4.8835e-02,  5.3169e+00,  1.1773e+00,  3.1735e-01, -4.4307e+01,\n",
       "         -7.2686e-01,  9.0037e-01,  3.8364e-01, -1.5992e+00,  9.7624e-01,\n",
       "          1.0184e+01, -1.0308e-03, -6.8410e-01,  1.3381e-01,  5.3241e-01,\n",
       "          3.7596e-02],\n",
       "        [ 1.1478e+00, -1.9792e+00,  4.0636e+00, -8.3352e+00,  5.9191e+00,\n",
       "         -8.9168e+00,  5.9597e+00, -3.6086e-01, -1.6060e+00,  2.2169e+00,\n",
       "         -1.3131e+00,  6.5857e-01,  4.7300e+00, -1.4259e+00, -1.3943e+00,\n",
       "         -2.7359e+00,  4.7301e-01, -5.8397e-01,  4.0359e+00,  3.1047e-01,\n",
       "         -4.0031e-01,  6.8685e+00,  5.5555e+00,  9.7637e+00, -3.0851e-01,\n",
       "         -1.9290e+00,  2.5749e+01, -1.0415e+01,  7.6860e+00, -5.3788e+00,\n",
       "          1.4164e+00, -2.1584e+00,  1.1486e+00,  6.5707e+00,  1.7147e+01,\n",
       "         -1.3797e+01,  8.1943e-01,  5.9200e+00, -3.1170e+01,  6.0275e+00,\n",
       "         -1.7803e+01, -9.4545e+00,  5.3206e+01, -5.5610e+00, -3.5660e+00,\n",
       "         -5.3316e+01, -2.1416e+00,  5.0526e-01, -7.7553e+00,  2.6556e+00,\n",
       "          2.8337e+00,  2.4897e+00,  2.5253e-01, -1.4594e+00, -4.7796e+01,\n",
       "         -2.4416e+00, -8.5952e-01, -6.5482e+00, -4.8739e+00,  1.4264e-01,\n",
       "          6.2303e+00, -2.0549e-01, -1.5748e-01,  2.1470e-01,  5.5530e-01,\n",
       "         -9.1976e-02],\n",
       "        [-1.5157e+00,  9.4121e-01,  1.3545e+00, -6.0158e-01, -1.1585e+01,\n",
       "         -3.1818e-02,  6.9978e+00, -1.7419e-01,  1.4597e+00,  5.9573e+00,\n",
       "         -2.6222e+00,  3.3498e-01,  6.5883e+00, -2.8867e+00,  3.2762e-01,\n",
       "         -1.0193e-01, -7.9735e-01, -3.8023e-01,  1.4692e+01,  1.8770e+00,\n",
       "         -1.1548e+00,  7.4672e+01,  1.1899e+01,  2.8805e+01, -3.7294e-01,\n",
       "         -4.0691e+00,  9.1351e+01,  6.2359e-02,  3.0676e+01, -1.5913e+01,\n",
       "          1.1596e+01, -1.5113e+00, -2.3825e+00,  7.1053e+01,  3.0428e+00,\n",
       "         -2.8324e+01, -6.8482e-01,  3.6834e+00, -9.7370e+01, -3.8427e+00,\n",
       "         -2.8877e+01,  8.8755e+00,  2.7082e+01, -1.7953e+01,  6.0505e+00,\n",
       "         -3.8914e+01, -1.1337e+00,  1.0853e+00,  3.8809e+00, -7.7546e+00,\n",
       "          3.9362e+00,  5.1531e+00,  2.8384e-01, -1.5278e-01, -3.6814e+01,\n",
       "         -3.1608e+00,  1.3188e-01,  5.9472e+00,  3.1222e+00,  2.2245e+00,\n",
       "          7.0218e+00, -2.7933e-02,  1.2984e+00, -5.5930e-02,  1.4987e-01,\n",
       "          4.3781e-01],\n",
       "        [ 2.8836e-01, -1.7706e+00,  5.2457e+00,  7.0875e+00, -1.0490e+01,\n",
       "          1.2562e+00,  8.7684e+00, -4.2568e-01, -4.3443e-01,  7.2404e+00,\n",
       "         -3.0977e-01,  3.1756e-03,  6.5014e+00, -4.1285e-01,  8.9840e-01,\n",
       "         -5.5573e-01, -8.0271e-01,  2.5493e-02,  2.3043e+00,  2.7932e+00,\n",
       "          5.8069e+00,  4.4540e+01, -5.3898e-01,  7.2577e+00,  5.5564e+00,\n",
       "         -3.6282e+00,  9.7338e+01, -1.1010e+01,  2.1074e+01, -1.1416e+01,\n",
       "          4.5613e+00, -2.0094e+00, -7.1136e+00,  5.5197e+01, -2.7544e+00,\n",
       "         -4.8266e+00,  3.0065e+00, -1.2972e+00, -9.3376e+01, -1.4199e+01,\n",
       "         -1.8919e+01,  8.2929e+00,  4.9632e+01, -1.1328e+01,  6.5851e+00,\n",
       "         -6.1093e+01,  1.9870e-02,  1.6715e+00, -2.4120e-01, -7.0665e+00,\n",
       "         -2.6515e+00,  1.1503e+00,  2.3608e-01,  1.0831e+00, -6.4718e+01,\n",
       "          1.6796e+00, -4.9863e+00,  2.9447e+00,  1.0342e+01,  4.1509e+00,\n",
       "          9.2133e+00,  9.1893e-01,  7.7881e-02,  2.9282e-01,  9.6707e-02,\n",
       "          2.3883e-01],\n",
       "        [-1.9550e+00, -2.9387e+00,  6.8778e+00,  4.7284e+00, -1.3579e+01,\n",
       "          5.2108e+00,  8.1874e+00,  1.1908e+00,  1.1172e+00,  3.7640e+00,\n",
       "          1.3029e+00, -3.6186e-01,  3.7078e+00,  1.8796e+00, -3.9144e-01,\n",
       "         -7.8080e-02,  9.1139e-01, -1.8259e-01,  9.2793e+00,  4.3357e+00,\n",
       "          3.9469e+00,  6.4398e+01, -5.7860e-01,  3.6139e+00,  8.3109e+00,\n",
       "          2.0525e+00,  4.4373e+01,  5.1842e+00,  2.1764e+01, -9.2769e+00,\n",
       "          7.5111e+00, -6.3562e+00, -6.1048e+00,  6.7612e+01, -7.2139e+00,\n",
       "          2.5673e-02,  8.3761e+00, -4.7324e+00, -4.0682e+01, -1.9545e+00,\n",
       "         -2.1806e+01,  6.3898e+00,  2.7648e+01, -1.3877e+01,  7.4889e+00,\n",
       "         -4.1159e+01,  1.2774e+00,  1.0150e+00,  4.0482e+00, -6.7868e+00,\n",
       "         -1.2300e+00,  3.2900e+00, -1.5722e-01, -1.6895e+00, -4.7144e+01,\n",
       "         -6.0769e-01, -2.1316e+00,  1.1582e+01,  7.0708e+00,  4.4446e+00,\n",
       "          8.7936e+00,  1.1399e+00,  1.2255e+00,  7.4744e-01,  4.7663e-01,\n",
       "         -7.6641e-02],\n",
       "        [ 6.6368e+00, -4.7995e-01,  5.7190e+00,  3.7148e+00, -1.0790e+01,\n",
       "          1.1556e+00,  1.5400e+01,  1.9629e+00, -3.5319e-01,  6.1540e+00,\n",
       "          1.2366e+00, -1.8135e-01,  3.2617e+00,  1.7477e+00,  8.9589e-01,\n",
       "         -1.7305e+00,  1.0979e-02,  4.1954e-01, -1.4716e-01,  3.8952e+00,\n",
       "          1.1589e+01,  4.8537e+01, -1.2384e+01,  1.6363e-01,  3.6026e+00,\n",
       "         -5.2386e+00,  1.0270e+02, -2.2017e+01,  8.8696e-01, -9.7100e+00,\n",
       "          3.2661e+00, -6.0390e+00, -1.0048e+01,  4.3689e+01,  9.4796e+00,\n",
       "         -1.3887e+01,  3.2426e+00,  1.7167e-01, -9.5424e+01, -2.3958e+01,\n",
       "         -6.0642e+00,  7.8497e+00,  7.9733e+01, -1.2236e+01,  8.4178e+00,\n",
       "         -8.3876e+01, -1.9585e+00,  4.1783e+00, -6.2127e+00, -7.8207e+00,\n",
       "         -3.5832e+00,  5.4313e+00, -2.0975e-01,  2.9752e-02, -8.7639e+01,\n",
       "          1.6780e+00, -5.5914e+00, -9.6479e-01,  1.2928e+01,  3.8729e+00,\n",
       "          7.3352e+00,  1.4484e+00,  7.9724e-01,  6.3155e-01,  1.3813e-01,\n",
       "          2.8975e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
